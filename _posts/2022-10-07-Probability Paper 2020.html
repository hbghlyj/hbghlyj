---
mathjax: true
tag: Probability
excerpt: A8 Probability Paper 2020
---
<ol><li>
<ol type="a"><li>
<ol type="i"><li>Let $V$ be a random variable. Define its moment generating function $M_V$.
</li>
<li>Derive the moment generating functions of random variables $Y$ and $Z$, where
<ul><li>$Y$ is Poisson distributed with parameter $Î» âˆˆ(0, âˆ)$,</li>
<li>and $Z$ is normally distributed with zero mean and variance $Ïƒ^2 âˆˆ(0, âˆ)$.</li></ul></li></ol>
</li>
<li>Consider a random variable $Y$ taking values in the non-negative integers, that is independent of a sequence $Z_n, n â©¾ 1$, of independent identically distributed random variables. Let
$$
R=\sum_{n=1}^Y Z_n
$$
with the convention that $R=0$ if $Y=0$.
<ol type="i"><li>Show that $M_R(t)=G_Y\left(M_{Z_1}(t)\right)$, where $G_Y(s)=M_Y(\log (s))$.
</li>
<li>Suppose further that $Z_n$ is normally distributed with zero mean and variance $Ïƒ^2 âˆˆ(0, âˆ)$ and that $Y$ is Poisson distributed with parameter $Î» âˆˆ(0, âˆ)$. Determine the moment generating function of $R$.</li></ol>
</li>
<li>
<ol type="i"><li>Define the notion of convergence in distribution for real-valued random variables.
</li>
<li>State the convergence theorem for moment generating functions.
</li>
<li>Consider a sequence $R_n, n â©¾ 1$, of independent random variables with the same distribution as $R$ in (b)(ii) and consider $S_n=R_1+â‹¯+R_n, n â©¾ 1$. Show that $S_n / \sqrt{n}$ converges in distribution. Determine the limiting distribution. If you use the Central Limit Theorem, you are expected to prove it.</li></ol>
</li>
<li>Let $Y$ be Poisson distributed with parameter $Î» âˆˆ(0, âˆ)$. Let $q âˆˆ(0,1)$. Show that there is $c âˆˆ(0, âˆ)$ such that $â„™(Y â©¾ m) â©½ c e^{-q m \log (m)}$ for all $m â©¾ 1$.</li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>State, without proof, the Strong Law of Large Numbers.
</li>
<li>State carefully, without proof, the transformation formula for bivariate probability density functions.</li></ol>
</li>
<li>
<ol type="i"><li>Let $X$ and $Y$ be independent exponentially distributed with parameter $Î» âˆˆ(0, âˆ)$. Show that $W=X+Y$ and $U=X /(X+Y)$ are independent, and determine their marginal distributions.
</li>
<li>Let $Z$ be Gamma distributed with probability density function $z e^{-z}$ for $z>0$. Calculate $â„™(Z>z)$ and $ğ”¼(1 / Z)$.
</li>
<li>Let $Z$ be as in (ii) and $X_0$ independent exponentially distributed with parameter 1 . By evaluating $â„™\left(X_0 / Z>v\right)$, or otherwise, find the probability density function of $V=X_0 / Z$.</li></ol>
</li>
<li>Consider a sequence $\left(X_n, n â©¾ 0\right)$ of independent exponentially distributed random variables with parameter 1 . Let
$$
R_n=X_{n-1} /\left(X_n+X_{n+1}\right), â€ƒ n â©¾ 1 .
$$
<ol type="i"><li>For $n â©¾ 1$, calculate the covariance of $R_n$ and $R_{n+2}$.
</li>
<li>Show that
$$
\frac{1}{n} \sum_{k=1}^n R_k â†’ 1 â€ƒ \text { almost surely, as } n â†’ âˆ \text {. }
$$</li></ol></li></ol>
</li></ol>
<h1>Solution</h1>
<ol><li>
<ol type="a"><li>
<ol type="i"><li>$M_V: â„ â†’[0, âˆ]$ is given by $M_V(t)=ğ”¼\left(e^{t V}\right)$.
</li>
<li>$M_Y(t)=\sum_{n=0}^{âˆ} e^{t n} \frac{Î»^n}{n !} e^{-Î»}=\exp \left(Î»\left(e^t-1\right)\right)$ by the exponential series. Also,
\begin{aligned}
M_Z(t) &=âˆ«_{-âˆ}^{âˆ} e^{t z} \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} e^{-z^2 / 2 Ïƒ^2} d z \\
&=e^{Ïƒ^2 t^2 / 2} âˆ«_{-âˆ}^{âˆ} \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} e^{-\left(z-t Ïƒ^2\right)^2 / 2 Ïƒ^2}=e^{Ïƒ^2 t^2 / 2}
\end{aligned}
as the pdf of the normal distribution with parameters $t Ïƒ^2$ and $Ïƒ^2$ integrates to 1.</li></ol>
</li>
<li>
<ol type="i"><li>Conditioning on $Y$, we get
\begin{aligned}
ğ”¼\left(e^{t R}\right) &=\sum_{n=0}^{âˆ} ğ”¼\left(\exp \left(t \sum_{k=1}^n Z_k\right)\right) â„™(Y=n) \\
&=\sum_{n=0}^{âˆ}\left(M_{Z_1}(t)\right)^n â„™(Y=n)=G_Y\left(M_{Z_1}(t)\right)
\end{aligned}
by independence of $Z_1, â€¦, Z_{\mathrm{n}}$ for each $n â‰¥ 1$, and where
$$
G_Y(s)=ğ”¼\left(s^Y\right)=ğ”¼\left(e^{Y \log (s)}\right)=M_Y(\log (s))
$$
</li>
<li>By (i) and (a)(ii), $M_R(t)=\exp \left(Î»\left(e^{Ïƒ^2 t^2 / 2}-1\right)\right)$.</li></ol>
</li>
<li>
<ol type="i"><li>$W_n â†’ W$ in distribution if $â„™\left(W_n â‰¤ w\right) â†’ â„™(W â‰¤ w)$ for all $w âˆˆ â„$ with $â„™(W=w)=0$.
</li>
<li>If there is $t_0>0$ such that $M_{W_n}(t), n â‰¥ 1$, and $M_W(t)$ are finite for all $t âˆˆ\left(-t_0, t_0\right)$, and if $M_{W_n}(t) â†’ M_W(t)$ for all $t âˆˆ\left(-t_0, t_0\right)$, then $W_n â†’ W$ in distribution.
</li>
<li>By (b)(ii) and by independence, we have for all $t âˆˆ â„$
$$
\log \left(M_{S_n / \sqrt{n}}(t)\right)=\log \left(\left(M_R(t / \sqrt{n})\right)^n\right)=n Î»\left(e^{Ïƒ^2 t^2 / 2 n}-1\right)
$$
and Taylor expansion yields
$$
\log \left(M_{S_n / \sqrt{n}}(t)\right)=Î» Ïƒ^2 t^2 / 2+o(1) â†’ Î» Ïƒ^2 t^2 / 2
$$
which is the logarithm of the moment generating function of the normal distribution with zero mean and variance $Î» Ïƒ^2$, by (a)(ii). By the convergence theorem in (ii), we conclude that $S_n / \sqrt{n}$ converges in distribution to this normal distribution.</li></ol>
</li>
<li>This is new. Students have seen bounds for SRW based on the same technique. We apply Markov's inequality and the moment generating function of $Y$ from (a)(ii):
$$
â„™(Y â‰¥ m)=â„™\left(e^{t Y} â‰¥ e^{t m}\right) â‰¤ e^{-t m+Î»\left(e^t-1\right)} â€ƒ \text { for all } t â‰¥ 0
$$
Optimising the exponent over $t â‰¥ 0$, we find the (minimal) bound when
$$
m=Î» e^t âŸº e^t=m / Î» âŸº t=\log (m / Î»)=\log (m)-\log (Î») .
$$
Hence $â„™(Y â‰¥ m) â‰¤ e^{-m \log (m)+m \log (Î»)+m-Î»} â‰¤ e^{-q m \log (m)}$ for $m$ sufficiently large, and a suitable factor $c âˆˆ[1, âˆ)$ suffices to also upper bound for the finitely many smaller $m$.</li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>Let $X_n, n â‰¥ 1$, be independent identically distributed with mean $Î¼=ğ”¼\left(X_1\right)$. Then $â„™\left(n^{-1} \sum_{1 â‰¤ k â‰¤ n} X_k â†’ Î¼\right)=1$.
</li>
<li>Let $D, R âŠ† â„^2$ and $T: D â†’ R$ a bijective transformation whose inverse is continuously differentiable with
$$
J(u, v)=\frac{âˆ‚ x}{âˆ‚ u} \frac{âˆ‚ y}{âˆ‚ v}-\frac{âˆ‚ x}{âˆ‚ v} \frac{âˆ‚ y}{âˆ‚ u}
$$
If $(X, Y)$ has joint pdf $f_{X, Y}: D â†’[0, âˆ)$, then $(U, V)=T(X, Y)$ is jointly continuous with pdf $f_{U, V}(u, v)=f_{X, Y}(x(u, v), y(u, v)){|J(u, v)|},(u, v) âˆˆ R$.</li></ol>
</li>
<li>
<ol type="i"><li>The transformation formula of (a)(ii) applies with $D=(0, âˆ)^2, R=(0, âˆ) Ã—(0,1), f_{X, Y}(x, y)=Î»^2 e^{-Î»(x+y)}$. The inverse of $T(x, y)=(x+y, x /(x+y))$ is $T^{-1}(w, u)=(w u, w(1-u))$ with Jacobian $J(w, u)=-u w-w(1-u)=-w$, so
$$
f_{W, U}(w, u)=Î»^2 e^{-Î» w} w, â€ƒ w âˆˆ(0, âˆ), u âˆˆ(0,1)
$$
This factorises into $f_W(w)=Î»^2 w e^{-Î» w}$ and $f_U(u)=1$, i.e. $W$ and $U$ are independent with $Wâˆ¼\operatorname{Gamma}(2, Î»)$ and $Uâˆ¼\operatorname{Uniform}(0,1)$.
</li>
<li>$â„™(Z>z)=âˆ«_z^{âˆ} w e^{-w} d w=\left[-w e^{-w}\right]_z^{âˆ}+âˆ«_z^{âˆ} e^{-w} d w=e^{-z}(1+z), z â‰¥ 0$. $ğ”¼(1 / Z)=âˆ«_0^{âˆ}(1 / z) z e^{-z} d z=1$
</li>
<li>$â„™\left(X_0 / Z>v\right)=âˆ«_0^{âˆ} âˆ«_{v z}^{âˆ} e^{-x} z e^{-z} d x d z=âˆ«_0^{âˆ} z e^{-z(1+v)} d z=(1+v)^{-2}$, $v>0$, by comparison with the Gamma$(2,1+v)$ pdf. By differentiation, $V=X_0 / Z$ has pdf $f_V(v)=2(1+v)^{-3}, v>0$.</li></ol>
</li>
<li><ol type="i"><li>By (b)(i), $X_1+X_2$ has the distribution of $Z$ for $Î»=1$. By (b)(ii), $ğ”¼(1 / Z)=1$. By independence,
$$
ğ”¼\left(R_1\right)=ğ”¼\left(\frac{X_0}{X_1+X_2}\right)=ğ”¼\left(X_0\right) ğ”¼\left(\frac{1}{Z}\right)=1
$$
By (b)(i), $X_2 /\left(X_1+X_2\right)$ has the uniform distribution of $U$ with mean $1 / 2$. Hence
$$
\operatorname{Cov}\left(R_n, R_{n+2}\right)=ğ”¼\left(\frac{X_0 X_2}{\left(X_1+X_2\right)\left(X_3+X_4\right)}\right)-1=ğ”¼\left(X_0\right) ğ”¼(U) ğ”¼(1 / Z)-1=-1 / 2 \text {. }
$$
</li>
<li>Note that $R_{3 n}$ depends on $X_{3 n-1}, X_{3 n}, X_{3 n+1}$, so $R_{3 n}, n â‰¥ 1$, are independent with $ğ”¼\left(R_{3 n}\right)=1$, by (i). By the SLLN of (a)(i),
$$
\frac{1}{n} \sum_{k=1}^n R_{3 k} â†’ 1 â€ƒ \text { with probability } 1
$$
Similarly, we have
$$
\frac{1}{n} \sum_{k=1}^n R_{3 k-1} â†’ 1 \text { and } \frac{1}{n} \sum_{k=1}^n R_{3 k-2} â†’ 1 â€ƒ \text { with probability } 1
$$
By Algebra of Limits for almost sure convergence, we have, with probability 1 ,
$$
\frac{1}{3 n} \sum_{k=1}^{3 n} R_k=\frac{1}{3 n} \sum_{k=1}^n R_{3 k-2}+\frac{1}{3 n} \sum_{k=1}^n R_{3 k-1}+\frac{1}{3 n} \sum_{k=1}^n R_{3 k} â†’ 1 .
$$
A simple sandwiching argument allows us to replace $3 n$ by $n$.</li></ol></li></ol>
</li></ol>