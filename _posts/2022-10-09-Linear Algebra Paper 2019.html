---
mathjax: true
tag: Linear Algebra
excerpt: A0 paper 2019
---
<ol><li>
<ol type="a"><li>Let $V$ be a finite dimensional vector space over $‚ÑÇ$ and let $T: V ‚Üí V$ be a linear transformation.
<ol type="i"><li>Define the minimal polynomial $m_T(x)$ of $T$. Show that $Œª ‚àà ‚ÑÇ$ is a root of $m_T(x)$ if and only if $Œª$ is an eigenvalue of $T$.
</li>
<li>Show that $m_T(x)$ has distinct roots if and only if $T$ is diagonalizable (i.e. $V$ has a basis consisting of eigenvectors of $T$).
</li>
<li>Assume that $\dim V>1$. Prove that there exists a linear transformation $B: V ‚Üí V$ such that for every polynomial $p(x) ‚àà ‚ÑÇ[x]$ we have $B ‚â† p(T)$.<br>
[You may use without proof properties of polynomials over fields and the Cayley-Hamilton theorem provided you state them clearly. If you use the Primary Decomposition Theorem you should prove it.]</li></ol>
</li>
<li>For a field $ùîΩ$ we denote by $S L(2, F)$ the set of 2 by 2 matrices with entries in F and having determinant 1.
<ol type="i"><li>Suppose that $A ‚àà S L(2, ‚ÑÇ)$ is diagonalizable. Show that $A=B^2$ for some $B ‚àà S L(2, ‚ÑÇ)$
</li>
<li>Show that the map $A ‚Ü¶ A^2$ for $A ‚àà S L(2, ‚ÑÇ)$ is not surjective onto $S L(2, ‚ÑÇ)$.
</li>
<li>Suppose now $ùîΩ=ùîΩ_p$ is a finite field of size $p$, where $p$ is a prime number. Does there exist $p$ such that the map $A ‚Ü¶ A^2$ with $A ‚àà S L\left(2, ùîΩ_p\right)$ is surjective onto $S L\left(2, ùîΩ_p\right)$ ? Justify your answer.</li></ol></li></ol>
</li>
<li>Let $V, W$ be two vector spaces over a field $ùîΩ$ and let $V'$ and $W'$ denote the dual spaces of $V$ and $W$ respectively. Let $T: V ‚Üí W$ be a linear transformation and let $T': W' ‚Üí V'$ be the dual of $T$.
<ol type="a"><li>Assume that both $V$ and $W$ are finite-dimensional spaces.
<ol type="i"><li>Let $B$ be a basis of $V$ and let $C$ be a basis of $W$. Define the dual basis $B'$ to $B$ in $V'$. Let $C'$ be the dual basis to $C$ in $W'$. Prove that the matrix $B_{B'}\left[T'\right]_{C'}$ is the transpose of $C[T]_B$.
</li>
<li>Suppose $V=W$. Show that $T$ and $T'$ have the same minimal polynomial and the same characteristic polynomial.
</li>
<li>For a subspace $U$ of $V$ define its annihilator $U^0$ in $V'$ and prove that $\dim U+\dim U^0=\dim V$.</li></ol>
</li>
<li>
<ol type="i"><li>Suppose $\dim V$ and $\dim W$ are both finite. Prove that $\dim \operatorname{Im}(T)=\dim \operatorname{Im}\left(T'\right)$.
</li>
<li>Now suppose $\dim V$ is infinite but $\dim W$ is finite. Is it still true that $\dim \operatorname{Im}(T)=\dim \operatorname{Im}\left(T'\right)$ ? Give a proof or a counterexample.</li></ol>
</li>
<li>Assume that $\dim V$ is finite. Let $f_1, ‚Ä¶, f_k ‚àà V'$. Show that $\left\{f_1, ‚Ä¶, f_k\right\}$ span $V'$ if and only if $\bigcap_{i=1}^k \ker f_i=\{0\}$.</li></ol>
</li>
<li>Let $V$ be a finite-dimensional inner product space over $‚ÑÇ$ and let $N: V ‚Üí V$ be a linear transformation.
<ol type="a"><li>For a subspace $U ‚äÜ V$, prove that $V=U ‚äï U^‚üÇ$.
[You may assume that $U$ has an orthonormal basis.]
</li>
<li>
<ol type="i"><li>Define the adjoint $N^*$ of $N$. [You don't have to show its existence or uniqueness.]
</li>
<li>Suppose that $U$ is a subspace of $V$ such that $N(U) ‚äÜ U$. Show that $N^*\left(U^‚üÇ\right) ‚äÜ U^‚üÇ$.
</li>
<li>Prove that $N$ can be written uniquely as a sum $N=S+A$ where $S^*=S$ and $A^*=-A$. Show that $N$ and $N^*$ commute (i.e. $N N^*=N^* N$) if and only if $S$ and $A$ commute.</li></ol>
</li>
<li>
<ol type="i"><li>Suppose that $N N^*=N^* N$. Show that $\ker N=\ker N^*$ and $\operatorname{Im}(N)=\operatorname{Im}\left(N^*\right)$.
</li>
<li>Suppose that ${‚ÄñN(v)‚Äñ}=\left‚ÄñN^*(v)\right‚Äñ$ for all $v ‚àà V$. Show that $N N^*=N^* N$.</li></ol></li></ol>
</li></ol>
<h1>Solution</h1>
<ol><li>
<ol type="a"><li>
<ol type="i"><li>The minimal polynomial $m_T(x)$ is defined to be the monic polynomial $f(x)$ of least degree such that $f(T)=0$. It exists since the Cayley-Hamilton theorem states that $œá(T)=0$ where $œá(x)=\det(x I-T)$ is the characteristic polynomial of $T$.
Now if $Œª$ is an eigenvalue of $T$ with eigenvector $v$, then $0=m_T(T)(v)=m_T(Œª) v$ and hence $m_T(Œª)=0$. Conversely if $Œª$ is a root of $m_T$ then $m_T=(x-Œª) g(x)$ with $\deg g(x)&lt;\deg m_T$. Therefore $g(T) ‚â† 0$ and we can find a nonzero vector $v$ such that $w:=g(T) v ‚â† 0$. But then $(T-Œª) w=(T-Œª) g(T) v=m_T(T) v=0$ and so $Œª$ is an eigenvalue of $T$ with eigenvector $w$.
</li>
<li>If $T$ is diagonalizable with respect to some basis $B$ so that $X={ }_B[T]_B$ is a diagonal matrix, we consider a polynomial $f(x)$ to be product of linear factors $(x-Œº)$ where $Œº$ ranges over the distinct diagonal entries of $X$. We see $f(T)=0$ and together with (a)(i) we deduce that $f=m_T$ and has distinct roots. Conversely suppose $m_T=\prod_{i=1}^k\left(x-Œº_k\right)$ has distinct roots. We argue by induction on $k$, the case $k=1$ being clear. Let $m_T(x)=\left(x-Œº_1\right) g(x)$. Then $g\left(Œº_1\right) ‚â† 0$. Let $U_1=\ker\left(T-Œº_1\right), U_2=\ker g(x)$. If $v ‚àà U_1 ‚à© U_2$ then $0=g(T) v=g\left(Œº_1\right) v$ and so $v=0$ since $Œº_1$ is not a root of $g(x)$. We now show $V=U_1+U_2$. Let $v ‚àà V$ and define $v_1=g\left(Œº_1\right)^{-1} g(T) v, v_2=v-v_1$. Since $m_T(v)=0=\left(T-Œº_1\right) g(T) v$ it follows that $T v_1=Œº_1 v$, i.e. $v_1 ‚àà U_1$. Also $g(T) v_2=g(T) v-g(T) v_1=g(T) v-g\left(Œº_1\right) v_1=0$ by the definition of $v_1$. So $v_2 ‚àà U_2$. Therefore $V=U_1 ‚äï U_2$. Now $T$ acts as the scalar $Œº_1$ on $U_1$ and $g(T)=0$ on $U_2$ hence the minimal polynomial of $T$ on $U_2$ has degree less than $k$. By induction we can choose a basis diagonalizing $T$ on $U_2$ and adding any basis of $U_1$ we are finished.
</li>
<li>Let $v$ be an eigenvector of $T$, and note that $v$ is still an eigenvector for any $p(T)$. We can extend $v$ to a basis $v, w_1, w_2 ‚Ä¶$ of $V$ and define $B(v)=w_1$ and $B\left(w_i\right)=0$. Then $v$ is not an eigenvector of $B$ and so $B ‚â† p(T)$ for any polynomial $p(x)$. The students can also argue using that the dimension of the space spanned by $\left\{1, T, T^2, ‚Ä¶,\right\}$ in $\operatorname{End}(V)$ is exactly deg $m_T ‚â§ n$.</li></ol>
</li>
<li>
<ol type="i"><li>If $A$ is diagonalizable, then for some change of basis matrix $P$ the matrix $P^{-1} A P$ is a diagonal matrix with eigenvalues $Œª, Œª^{-1}$ for some nonzero $Œª ‚àà ‚ÑÇ$. Take $Œº ‚àà ‚ÑÇ$ such that $Œº^2=Œª$ and let $X$ be the diagonal matrix with entries $Œº, Œº^{-1}$. Take $B=P^{-1} X P$.
</li>
<li>Take $A=\left(\begin{array}{cc}-1&1 \\ 0&-1\end{array}\right)$. Suppose $A=B^2$. The eigenvalues of $B$ must be $¬± i$ and since $\det(B)=1$ they must be distinct: $i$ and $-i$. By part (a) $B$ is similar to a diagonal matrix with diagonal entries $\{i,-i\}$ and so $B^2=-I d ‚â† A$. Contradiction.
</li>
<li>SL$(2, ùîΩ)$ is a finite set so if the map $A ‚Ü¶ A^2$ is surjective it must be injective. But this is not true, as for odd $p$ we have $I d^2=(-I d)^2$ while if $p=2$
then Id$^{2}=\left(\begin{array}{ll}1&1 \\ 0&1\end{array}\right)^{2}$</li></ol></li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>Suppose $B=\left\{b_1, ‚Ä¶, b_n\right\}$ is a basis of $V$ and $C=\left\{c_1, ‚Ä¶, c_m\right\}$ is a basis of $W$. We define $B'=\left\{b_1', ‚Ä¶, b_n'\right\}$ where $b_i' ‚àà V'$ is such that $b_i'\left(b_j\right)=Œ¥_{i j}$.<br>Similarly we take $C'=\left\{c_1', ‚Ä¶, c_m'\right\}$. Suppose $_C[T]_B=\left(a_{i j}\right)$ so that $T\left(b_j\right)=\sum_{i=1}^m a_{i j} c_i$. We compute $T'\left(c_j'\right)\left(b_s\right)=c_j'\left(T\left(b_s\right)\right)=c_j'\left(\sum_{i=1}^m a_{i s} c_i\right)=a_{j s}$ This gives $T'\left(c_j'\right)=\sum_{i=1}^n a_{j i} b_i'$ and so $_{B'}\left[T'\right]_{C^r}$ is the transpose of $_C[T]_B$.
</li>
<li>For a polynomial $f(x)$ and a square matrix $X$ we have $f\left(X^t\right)=(f(X))^t$ and so $f(T)=0$ if and only if $f\left(X^t\right)=0$. It follows that $m_T=m_{T'}$. Let $A=B[T]_B$. Then $œá_T(x)=\det(x \text{Id}-A)=\det(x\text{Id}-A)^t=\det\left(x \text{Id}-A^t\right)=œá_{T^r}(x)$.
</li>
<li>We define $U^0:=\left\{f ‚àà V' ‚à£ f(u)=0,‚àÄ u ‚àà U\right\}$.
Let $b_1, ‚Ä¶, b_k$ be a basis of $U$ and extend this to a basis $B=\left\{b_1, ‚Ä¶, b_n\right\}$ of $V$. We claim that $U^0$ has basis $b_{k+1}', ‚Ä¶, b_n'$. Indeed these functionals are linearly independent (since they are a subset of $B'$), and for $f=\sum_{i=1}^n Œ±_i b_i'$ the condition $f ‚àà U^0$ is equivalent to $f\left(b_i\right)=0$ for $i=1, ‚Ä¶, k$, which is equivalent to $Œ±_1=‚ãØ=Œ±_k=0$. This proves the claim. Hence $\dim U^0=n-k=\dim V-\dim U$ and we are done.</li></ol>
</li>
<li>
<ol type="i"><li>There are many ways to argue this, here is an argument which also applies to (ii).<br>We note that $(\operatorname{Im}(T))^0=\ker T'$. Indeed $f ‚àà(\operatorname{Im}(T))^0$ iff $f(T v)=0$ for all $v ‚àà V$ iff $f ‚àò T=0$ iff $f ‚àà \ker T'$. Now by part (a) (iii) and the Rank-Nullity theorem applied to $T'$ we have
$$
\dim \operatorname{Im}(T)=\dim W-\dim(\operatorname{Im}(T))^0=\dim W'-\dim \ker T'=\dim \operatorname{Im}\left(T'\right)
$$
</li>
<li>The above argument only uses that $\dim W$ is finite, so the result remains true even if $\dim V$ is infinite.</li></ol>
</li>
<li>Let $U=\bigcap_{i=1}^k\ker f_i$ and let $L$ be the subspace of $V'$ spanned by all $f_i$. Observe that $U=\bigcap_{h ‚àà L}$ ker $h$. Now if $L=V'$ then choosing a basis $B$ of $V$ we consider the dual basis $B'$ and then $U=\bigcap_{b ‚àà B'} \ker b=\{0\}$.<br>For the converse the students may argue using the natural isomorphism between $V$ and $V''$. Here is an alternative short argument: Suppose $\dim V=n$ and $L ‚â† V'$. Choose a basis $g_1, ‚Ä¶, g_k$ of $L$ and note $k&lt;n$. Thus
$$
U=\bigcap_{i=1}^k \ker g_i=\ker œï
$$
where $œï: V ‚Üí \mathrm{F}^k$ is the linear map $œï(v)=\left(g_1(v), g_2(v), ‚Ä¶, g_k(v)\right)$. Since $k&lt;\dim V$ the Rank-Nullity Theorem applied to $œï$ gives that $U=\ker œï ‚â†\{0\}$. Contradiction, therefore $L=V'$.</li></ol>
</li><li><ol type="a"><li>If $v ‚àà U ‚à© U^‚üÇ$ then $‚ü®v, v‚ü©=0$ and hence $v=0$ since the inner product is positive definite. Hence $U ‚à© U^‚üÇ=\{0\}$. We now show that $V=U+U^‚üÇ$.<br>Let $v ‚ààV$. Let $e_1, ‚Ä¶, e_k$ be an orthonormal basis of $U$ and define $v_1=\sum_{i=1}^k\left< v, e_i\right> e_i$. Then $v_1 ‚àà U$ and $\left< v, e_i\right>=\left< v_1, e_i\right>$ for all $i$ which implies that $v-v_1$ is orthogonal to each $e_i$, i.e. $v-v_1 ‚àà U^‚üÇ$. Hence $V=U+U^‚üÇ$ and therefore $V=U ‚äï U^‚üÇ$.
</li>
<li>
<ol type="i"><li>The adjoint $N^*$ is the unique linear transformation $N^*: V ‚Üí V$ such that $\left< N^*(v), w\right>=‚ü®v, N(w)‚ü©$ for all $v, w ‚àà V$.
</li>
<li>Fix $w ‚àà U^‚üÇ$ and let $v ‚àà U$. We have $\left< N^*(w), v\right>=‚ü®w, N(v)‚ü©=0$ since $N(v) ‚àà U$. This holds for all $v ‚àà U$ and hence $N^*(w) ‚àà U^‚üÇ$. The vector $w ‚àà U^‚üÇ$ was arbitrary and so $N^*\left(U^‚üÇ\right) ‚äÜ U^‚üÇ$
</li>
<li>If $N=S+A$ as required then $N^*=S-A$ and so we can solve $S=\left(N+N^*\right) / 2, A=\left(N-N^*\right) / 2$. This $A$ and $S$ are uniquely determined by $N$. Conversely we check $\left(\frac{N+N^*}{2}\right)^*=\frac{N+N^*}{2}$ and $\left(\frac{N-N^*}{2}\right)^*=-\frac{N-N^*}{2}$ so $A$ and $S$ exist for any $N$.
Now if $N N^*=N^* N$ then we check
$$
\frac{N+N^*}{2} \frac{N-N^*}{2}=\frac{N^2-\left(N^*\right)^2}{2}=\frac{N-N^*}{2} \frac{N+N^*}{2}
$$
Conversely if $A$ and $S$ commute than $N N^*=(A+S)(S-A)=S^2-A^2=(S-A)(A+S)=N^* N$</li></ol>
</li>
<li>
<ol type="i"><li>Suppose $v ‚àà \ker N$. Then $\left\|N^*(v)\right\|^2=\left< N^* v, N^* v\right>=\left< v, N N^* v\right>=\left< v, N^* N v\right>=0$ and so $N^*(v)=0$ giving that $v ‚àà$ ker $N^*$. Hence ker $N ‚äÜ$ ker $N^*$ The same argument applied with $N^*$ instead of $N$ gives the opposite containment and hence ker $N^*=$ ker $N$.<br>
Let $U=\ker N=\ker N^*$. Since both $N$ and $N^*$ send $U$ into $U$, from part (b) (ii) we have $N\left(U^‚üÇ\right) ‚äÜ U^‚üÇ$ and $N^*\left(U^‚üÇ\right) ‚äÜ U^‚üÇ$. Also both $N$ and $N^*$ are injective when restricted to $U^‚üÇ$ and hence both maps are bijections when restricted to $U^‚üÇ$. Finally $N(V)=N\left(U+U^‚üÇ\right)=N\left(U^‚üÇ\right)=U^‚üÇ$ and arguing with $N^*$ in place of $N$ we get $\operatorname{Im}(N)=\operatorname{Im}\left(N^*\right)=U^‚üÇ$.
</li>
<li>We have $\left\|N^*(v)\right\|=\left< v, N N^*(v)\right>$ and ${\|N(v)\|}=\left< v, N^* N(v)\right>$. Therefore if we set $A=N N^*-N^* N$ we get $‚ü®v, A v‚ü©=0$ for all $v ‚àà V$. From this point the students can argue with the spectral theorem to deduce $A=0$ but there is a direct way: Let $u, v ‚àà V$ and apply the above equality to $u+v$. So $0=‚ü®u+v, A(u+v)‚ü©$. Using $‚ü®v, A v‚ü©=‚ü®u, A(u)‚ü©=0$ we obtain
$$
‚ü®u, A(v)‚ü©+‚ü®v, A(u)‚ü©=0
$$
Now replace $v$ with $iv$ to obtain
$$
i‚ü®u, A(v)‚ü©-i‚ü®v, A(u)‚ü©=0
$$
Solving the two equations we get $‚ü®u, A(v)‚ü©=0$ for all $u, v ‚àà V$ and so $A=0$ and $NN^*=N^* N$.</li></ol></li></ol></li>
</ol>