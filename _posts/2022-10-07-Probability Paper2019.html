---
mathjax: true
tag: Probability
excerpt: A8 Paper 2019
---
<ol><li>Throughout this question let the function $f: â„^2 â†’ â„$ be given by
$$
f(x, y)= \begin{cases}c x(y-x) e^{-y}, & 0&lt;x&lt;y \\ 0, & \text { otherwise }\end{cases}
$$
Throughout this question you may use standard properties of sums of independent random variables without proof.
<ol type="a"><li>
<ol type="i"><li>Determine the value of $c âˆˆ(0, âˆ)$ for which $f$ is a joint probability density function.
Consider a pair $(X, Y)$ of random variables with joint probability density function $f$.
</li>
<li>Justifying briefly, are $X$ and $Y$ independent?
</li>
<li>Determine the marginal distributions of $X$ and $Y$, and find their means and variances.
</li>
<li>Determine the conditional distribution of $X$ given $Y=y$, for any $y>0$.
</li>
<li>Find the covariance of $X$ and $Y$.</li></ol>
</li>
<li>
<ol type="i"><li>State, without proof, the Central Limit Theorem.
</li>
<li>Let $\left(X_j, Y_j\right), j â©¾ 1$, be a sequence of independent pairs with joint probability density function $f$. Let $S_n=X_1+â‹¯+X_n$ and $T_n=Y_1+â‹¯+Y_n, n â©¾ 1$. Show that
$$
â„™\left(S_n â©½ 2 n\right) â†’ \frac{1}{2} â€ƒ \text { and } â€ƒ â„™\left(T_n â©½ 4 n\right) â†’ \frac{1}{2}, â€ƒ \text { as } n â†’ âˆ .
$$
</li>
<li>Find the limit as $n â†’ âˆ$ of $â„™\left(S_n â©½ 2 n, T_n â©½ 4 n\right)$.</li></ol></li></ol>
</li>
<li>
<ol type="a"><li>State the Strong Law of Large Numbers, without proof.
</li>
<li>Let $p âˆˆ[1 / 2,1]$ and consider the simple random walk on $â„¤$ with transition probabilities $p_{i, i+1}=1-p_{i, i-1}=p$.
<ol type="i"><li>Determine, for each $p âˆˆ[1 / 2,1]$, the communicating classes of this Markov chain.
</li>
<li>Suppose that $S_0=a âˆˆ â„¤$. For each $p âˆˆ[1 / 2,1]$, does the Strong Law of Large Numbers determine $\lim _{n â†’ âˆ} S_n$ ? If so, find this limit, justifying your answer.
</li>
<li>Suppose that $S_0=a âˆˆ â„¤$. For each $p âˆˆ[1 / 2,1]$, find $h:=â„™\left(S_n=a\text{ for some }n â©¾ 1\right)$.</li></ol>
</li>
<li>Now let $p âˆˆ(0,1 / 2]$ and consider the Markov chain on $\{0,1,2, â€¦\}$ with transition probabilities $q_{i, i+1}=1-q_{i, i-1}=p$ for $i â©¾ 1$ and $q_{0,1}=1$.
<ol type="i"><li>Show that this Markov chain is recurrent.
</li>
<li>Calculate all stationary distributions.</li></ol>
</li>
<li>Now let $p^{+} âˆˆ(0,1 / 2)$ and $p^{-} âˆˆ(1 / 2,1)$. Consider the Markov chain with transition probabilities $r_{i, i+1}=1-r_{i, i-1}=p^{+}, i â©¾ 1, r_{i, i+1}=1-r_{i, i-1}=p^{-}, i â©½-1$, and $r_{0,1}=r_{0,-1}=1 / 2$. Calculate the stationary distribution.</li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>Let $r, Î» âˆˆ(0, âˆ)$. Determine the moment generating function of the Gamma distribution with probability density function
$$
f(x)= \begin{cases}\frac{1}{Î“(r)} Î»^r x^{r-1} e^{-Î» x}, & x>0 \\ 0, & \text { otherwise. }\end{cases}
$$
</li>
<li>Let $E_1, â€¦, E_n$ be independent exponentially distributed with parameter $Î» âˆˆ(0, âˆ)$. Determine the distribution of $E_1+â‹¯+E_n$.</li></ol>
</li>
<li>
<ol type="i"><li>Define what is meant by a Poisson process $\left(N_t, t â©¾ 0\right)$ of rate $Î» âˆˆ(0, âˆ)$.
</li>
<li>Suppose that arrivals of buses are modelled by a Poisson process with rate $Î»$. Based on the definition you provided in (i), derive the distribution of the number of arrivals to time $t>0$, and determine the distribution of the $n$th arrival time $T_n, n â©¾ 1$.</li></ol>
</li>
<li>State, without proof, the theorem about superposition of Poisson processes.
</li>
<li>Let $\left(L_t, t â©¾ 0\right)$ be a random process starting from $L_0=0$ with independent increments and $L_t âˆ¼ \operatorname{Poisson}\left(Î± t^m\right)$ for some $Î± âˆˆ(0, âˆ)$ and $m>1$. We refer to such a process as an inhomogeneous Poisson process with rate function $Î¼(t)=Î± m t^{m-1}, t â©¾ 0$.
<ol type="i"><li>Determine the distribution of the first arrival time $S_1$ of $\left(L_t, t â©¾ 0\right)$.
</li>
<li>By considering events of the form $\left\{S_1 â©½ t, S_2>u\right\}$, or otherwise, determine the joint distribution of $S_1$ and $S_2-S_1$, where $S_2$ is the second arrival time of $\left(L_t, t â©¾ 0\right)$. Comment on your findings.
</li>
<li>Let $\left(N_t, t â©¾ 0\right)$ be a Poisson process of rate $Î» âˆˆ(0, âˆ)$ independent of $\left(L_t, t â©¾ 0\right)$. With brief justification, find a superposition theorem for $\left(L_t, t â©¾ 0\right)$ and $\left(N_t, t â©¾ 0\right)$.</li></ol></li></ol>
</li></ol>
<h1>Solution</h1>
<ol><li>
<ol type="a"><li>
<ol type="i"><li>$âˆ«_x^{âˆ} f(x, y) d y=âˆ«_0^{âˆ} c x z e^{-x-z} d z=c x e^{-x}$ recognising the integral for the mean of a standard exponential distribution. Similarly, integrating over $x âˆˆ(0, âˆ)$, we obtain integral $c$, so we need $c=1$.
</li>
<li>$X$ and $Y$ are not independent since the probability density function does not factorise into a function of $x$ and a function of $y$.
</li>
<li>From (i), $f_X(x)=âˆ«_x^{âˆ} f(x, y) d y=x e^{-x}, x>0$. So, $X âˆ¼ \operatorname{Gamma}(2,1)$. Similarly, $f_Y(x)=âˆ«_0^y x(y-x) e^{-y} d x=e^{-y}\left[\frac{1}{2} x^2 y-\frac{1}{3} x^3\right]_0^y=\frac{1}{6} y^3 e^{-y}, y>0$, so $Y$ is Gamma$(4,1)$. Hence, as sums of independent exponential variables with mean and variance 1, we have $ğ”¼(X)=\operatorname{Var}(X)=2, ğ”¼(Y)=\operatorname{Var}(Y)=4$, by independence.
</li>
<li>$f_{X âˆ£ Y=y}(x)=f(x, y) / f_Y(y)=6 x(y-x) / y^3, x âˆˆ(0, y)$.
</li>
<li>$ğ”¼(X Y)=âˆ«_0^{âˆ} âˆ«_0^y x^2 y(y-x) e^{-y} d x d y=âˆ«_0^{âˆ}\left(\frac{1}{3} y^5-\frac{1}{4} y^5\right) e^{-y} d y=10$ integral of Gamma$(6,1)$ density, noting that $Î“(6)=5 !=120$. Hence $\operatorname{Cov}(X, Y)=ğ”¼(X Y)-ğ”¼(X) ğ”¼(Y)=2$. Students may also represent $Y=X+X'$ for $X' âˆ¼\operatorname{Gamma}(2,1)$ independent of $X$ and $\operatorname{Cov}(X, Y)=\operatorname{Var}(X)=2$.</li></ol>
</li>
<li>
<ol type="i"><li>Let $\left(X_j, j â‰¥ 1\right)$ be a sequence of independent identically distributed random variables with mean $Î¼$ and variance $Ïƒ^2 âˆˆ(0, âˆ)$. Let $S_n=X_1+â‹¯+X_n$, $n â‰¥ 1$. Then
$$
â„™\left(\frac{S_n-n Î¼}{\sqrt{n Ïƒ^2}} â‰¤ x\right) â†’ âˆ«_{-âˆ}^x \frac{1}{\sqrt{2 Ï€}} e^{-z^2 / 2} d z=â„™(Z â‰¤ x) .
$$
</li>
<li>The Central Limit Theorem applies since $X$ and $Y$ have positive finite variance, by (a)(iii). By symmetry $â„™(Z â‰¤ 0)=1 / 2$. Hence
$$
\begin{aligned}
& â„™\left(S_n â‰¤ 2 n\right)=â„™\left(\frac{S_n-n ğ”¼(X)}{\sqrt{n \operatorname{Var}(X)}} â‰¤ 0\right) â†’ \frac{1}{2} \\
& â„™\left(T_n â‰¤ 4 n\right)=â„™\left(\frac{T_n-n ğ”¼(Y)}{\sqrt{n \operatorname{Var}(Y)}} â‰¤ 0\right) â†’ \frac{1}{2} .
\end{aligned}
$$
</li>
<li>$S_n$ and $T_n$ are not independent, but writing $T_n=S_n+S_n'$ for independent $S_n$ and $S_n'$, we have that $\left(S_n-2 n\right) / \sqrt{2 n}$ and $\left(S_n'-2 n\right) / \sqrt{2 n}$ converge in distribution to independent normals $Z$ and $Z'$, say, and hence
$$
\begin{aligned}
â„™\left(S_n â‰¤ 2 n, T_n â‰¤ 4 n\right) & =â„™\left(\frac{S_n-2 n}{\sqrt{2 n}} â‰¤ 0, \frac{S_n-2 n}{\sqrt{2 n}}+\frac{S_n'-2 n}{\sqrt{2 n}} â‰¤ 0\right) \\
& â†’ â„™\left(Z&lt;0, Z+Z'&lt;0\right)=â„™\left(Î˜ âˆˆ\left(Ï€, \frac{7}{4} Ï€\right)\right)=3 / 8,
\end{aligned}
$$
by spherical symmetry of the standard bivariate normal distribution, which makes the angular part $Î˜ âˆ¼ \operatorname{Unif}(0,2 Ï€)$.</li></ol></li></ol>
</li>
<li>
<ol type="a"><li>Let $\left(X_j, j â‰¥ 1\right)$ be a sequence of independent identically distributed random variables with finite mean $Î¼$ and $S_n=X_1+â‹¯+X_n, n â‰¥ 1$. Then $â„™\left(S_n / n â†’ Î¼\right)=1$.
</li>
<li>
<ol type="i"><li>For $p âˆˆ[1 / 2,1)$, the only communicating class is $â„¤$, while for $p=1$, all $\{n\}$, $n âˆˆ â„¤$ are singleton classes.
</li>
<li>We can write $S_n=a+X_1+â‹¯+X_n$ for $â„™\left(X_j=1\right)=1-â„™\left(X_j=-1\right)=p$, with $ğ”¼\left(X_j\right)=2 p-1$. For $p>1 / 2$, SLLN yields $\left(S_n-a\right) / n â†’2p-1>0$ hence $S_n â†’ âˆ$ a.s. For $p=1 / 2$ the SLLN does not determine the limiting behaviour.
</li>
<li>By translation invariance we may assume $a=0$. Let
$$
h_m=â„™_m\left(S_n=0 \text { for some } n â‰¥ 0\right), â€ƒ m âˆˆ â„¤ .
$$
Then $\left(h_m, m âˆˆ â„¤\right)$ is the minimal solution of $h_0=1, h_m=p h_{m+1}+(1-p) h_{m-1}$. Let $p=1 / 2$. Then, $h_1&lt;1$ would imply $h_{-1}>1$, which is absurd, so $h_1=1$. Inductively, $h_m=1$ for all $m âˆˆ â„¤$.
Let $p>1 / 2$. Then, $h_m=1$ for all $m&lt;0$ by (ii). For $m>0$, try $h_m=Î²^m$. Then $Î²=p Î²^2+(1-p)$, factorising $Î²^2-Î² / p+(1-p) / p=(Î²-1)(Î²-(1-p) / p)$, we see that $Î²=(1-p) / p$ yields the minimal solution $h_m=((1-p) / p)^m, m â‰¥ 0$. Conditioning on the first step and applying the Markov property, we find
$$
h=p h_1+(1-p) h_{-1}=1-p+1-p=2-2 p .
$$</li></ol>
</li>
<li>
<ol type="i"><li>By irreducibility, we only need to check that 0 is recurrent. By symmetry, (b) (iii) for $p âˆˆ(0,1 / 2]$, yields $h_m=1$ for all $m â‰¥ 1$. Here, conditioning on the first step and applying the Markov property, we find that the return probability to 0 is $h'=h_1=1$, so 0 is recurrent.
</li>
<li>$Î¾ Q=Î¾$ is equivalent to $Î¾_m=Î¾_{m-1} p+Î¾_{m+1}(1-p)$. Try $Î¾_m=A Î³^m$, then, as above $Î³ âˆˆ\{p /(1-p), 1\}$. Only $h_m=A(p /(1-p))^m$ can be normalised, and only for $p&lt;1 / 2$, to satisfy $\sum_{m â‰¥ 0} Î¾_m=1$, and $A=1-p /(1-p)$ yields a geometric distribution with parameter $A=(1-2 p) /(1-p)$. For $p=1 / 2$ there is no solution satisfying $\sum_{m â‰¥ 0} Î¾_m=1$, so there is no stationary distribution.</li></ol>
</li>
<li>In (c)(ii), the mean return time to 0 is $1 / Î¾_0=(1-p) /(1-2 p)$. For (d), denote the stationary distribution by $Î·$. Conditioning on the first step, the mean return time to 0 here is $m_0=1+\frac{1}{2}\left(a^{+}+a^{-}\right)$ where $a^{+}=\left(1-p^{+}\right) /\left(1-2 p^{+}\right)-1=p^{+} /\left(1-2 p^{+}\right)$ and $a^{-}=p^{-} /\left(2 p^{-}-1\right)-1=\left(1-p^{-}\right) /\left(2 p^{-}-1\right)$, and $Î·_0=1 / m_0$. By the Ergodic Theorem, this is also the long-term proportion of time spent in 0. Of the time not spent at 0, a proportion $a^{+} /\left(a^{+}+a^{-}\right)$ is spent positive since each return from 1 takes $a^{+}$ on average, each return from $-1$ takes $a^{-}$. By (c)(ii), these proportions further split into proportions $Î¾_m /\left(1-Î¾_0\right)$ spent in $m$. Hence
$$
Î·_m=\left(1-Î·_0\right) \frac{a^{+}}{a^{+}+a^{-}} \frac{1-2 p^{+}}{1-p^{+}}\left(\frac{p^{+}}{1-p^{+}}\right)^{m-1}=\frac{1}{2+a^{+}+a^{-}}\left(\frac{p^{+}}{1-p^{+}}\right)^m
$$
for $m â‰¥ 1$, and by symmetry, $Î·_m=\left(1 /\left(2+a^{+}+a^{-}\right)\right)\left(\left(1-p^{-}\right) / p^{-}\right)^{|m|}$ for $m â‰¤-1$.</li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>$M(t)=âˆ«_0^{âˆ} e^{t x} f(x) d x=âˆ«_0^{âˆ} \frac{1}{Î“(r)} Î»^r x^{r-1} e^{-(Î»-t) x} d x=\left(\frac{Î»}{Î»-t}\right)^r$, for $t&lt;Î»$, as the Gamma$(r, Î»-t)$ density integrates to 1.
</li>
<li>$\operatorname{Exp}(Î»)=\operatorname{Gamma}(1, Î»)$. By independence, for all $t&lt;Î»$,
$$
ğ”¼\left(e^{t\left(E_1+â‹¯+E_n\right)}\right)=ğ”¼\left(e^{t E_1}\right) â‹¯ ğ”¼\left(e^{t E_n}\right)=\left(\frac{Î»}{Î»-t}\right)^n .
$$
By the uniqueness theorem for moment generating functions, $E_1+â‹¯+E_n âˆ¼\operatorname{Gamma}(n, Î»)$.</li></ol>
</li>
<li>
<ol type="i"><li>$N_0=0$, increments $N_{t_j}-N_{t_{j-1}}, 1 â‰¤ j â‰¤ n$, are independent for all $0=t_0&lt;t_1&lt;â‹¯&lt;t_n$, and $N_{s+t}-N_s âˆ¼\operatorname{Poisson}(Î» t)$ for all $s, t â‰¥ 0$.
</li>
<li>By definition, for $s=0$, we have $N_t=N_{0+t}-N_0 âˆ¼ \operatorname{Poisson}(Î» t)$.
\[â„™\left(T_n>t\right)=â„™\left(N_t â‰¤ n-1\right)=\sum_{k=0}^{n-1} \frac{(Î» t)^k}{k !} e^{-Î» t} \]Hence, differentiation yields\[
f_{T_n}(t)=-\frac{d}{d t} â„™\left(T_n>t\right)=\sum_{k=0}^{n-1}\left(\frac{Î»^{k+1} t^k}{k !}-\frac{k Î»^k t^{k-1}}{k !}\right) e^{-Î» t}=\frac{1}{Î“(n)} Î»^n t^{n-1} e^{-Î» t}\]
for all $t>0$. Hence, $T_n âˆ¼ \operatorname{Gamma}(n, Î»)$.</li></ol>
</li>
<li>Let $\left(M_t, t â‰¥ 0\right)$ and $\left(N_t, t â‰¥ 0\right)$ be two independent Poisson processes of rates $Î¼$ and $Î»$. Then $K_t=M_t+N_t, t â‰¥ 0$, is a Poisson process of rate $Î¼+Î»$.
</li>
<li>
<ol type="i"><li>$â„™\left(T_1>t\right)=â„™\left(L_t-L_0=0\right)=\exp \left(-Î± t^m\right)$.
Hence $f_{T_1}(t)=Î± m t^{m-1} \exp \left(-Î± t^m\right), t>0$.
</li>
<li>First note that $N_u-N_t âˆ¼ \operatorname{Poisson}\left(Î±\left(u^m-t^m\right)\right)$, by independence of increments.
$$
\begin{aligned}
â„™\left(S_1 â‰¤ t, S_2>u\right) & =â„™\left(L_t=1, L_u-L_t=0\right)=Î± t^m e^{-Î± t^m} e^{-Î±\left(u^m-t^m\right)} \\
& =Î± t^m e^{-Î± u^m},
\end{aligned}
$$
for $0&lt;t&lt;u$.
By differentiation, $f_{S_1, S_2}(t, u)=Î± m t^{m-1} Î± m u^{m-1} e^{-Î± u^m}, 0&lt;t&lt;u$.
By the transformation formula, $f_{S_1, S_2-S_1}(t, s)=Î±^2 m^2 t^{m-1}(s+t)^{m-1} e^{-Î±(s+t)^m}$, $t>0, s>0$.
As $m>1$, this does not factorise, so $S_1$ and $S_2-S_1$ are not independent, in contrast to the case $m=1$ of a (homogeneous) Poisson process.
</li>
<li>$J_t=L_t+N_t, t â‰¥ 0$, is an inhomogeneous Poisson process with rate function $Î± m t^{m-1}+Î»$. This is because independent increments are preserved as for the standard superposition theorem. Clearly $J_0=0$, and the rate function, which is the derivative of the parameter of the Poisson distribution of $J_t$ is identified from $J_t=L_t+N_t âˆ¼ \operatorname{Poisson}\left(Î± t^m+Î» t\right)$.</li></ol></li></ol>
</li></ol>
