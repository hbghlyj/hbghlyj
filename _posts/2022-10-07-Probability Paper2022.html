---
mathjax: true
tag: Probability
excerpt: A8 Probability Paper 2022
---
<style>.q{width: fit-content;padding: 6px;border-width: 2px;border-style: solid;border-color: #0073CF;background-color: #F5FFFA;}</style>
<div class="q">
<ol type="a"><li>Let $Z_n,n‚©æ1$, be random variables and $c‚àà‚Ñù$. Show that $Z_n‚Üíc$ in probability if and only if $Z_n‚Üíc$ in distribution.</li><li>Fix $Œª‚àà(0,‚àû)$. For each $r‚àà(0,‚àû)$, consider a random variable $X_r$ with probability density function
$$
f_{r,Œª}(x)=\begin{cases}\frac{1}{Œì(r)}x^{r-1}Œª^r e^{-Œª x},&x‚àà(0,‚àû),\\0,&\text { otherwise. }\end{cases}
$$
Recall that this means that $X_r$ has the Gamma distribution with shape parameter $r$ and rate parameter $Œª$.
<ol type="i">
<li>Carefully derive the moment generating function of $X_r$.</li>
<li>
Show that $X_r/r$ converges in distribution as $r‚Üí‚àû$. Does this convergence hold in probability?<br>
[You may use standard theorems about moment generating functions without proof.]
</li></ol></li>
<li><ol type="i"><li>Define the Poisson process $\left(N_t,t‚©æ0\right)$ of rate $Œª‚àà(0,‚àû)$ in terms of properties of its increments over disjoint time intervals.</li><li>Show that the first arrival time $T_1=\inf\left\{t‚©æ0:N_t=1\right\}$ is exponentially distributed with parameter $Œª‚àà(0,‚àû)$.</li><li>Show that $T_n=\inf\left\{t‚©æ0:N_t=n\right\}$ has a Gamma distribution for all $n‚©æ1$.<br>
[If you use the inter-arrival time definition of the Poisson process, you are expected to prove that it is equivalent to the definition given in (i).]</li><li>Let $R_n,n‚©æ1$, be independent Gamma-distributed random variables. Let $R_n$ have shape parameter $n$ and rate parameter $Œª$.<br>
Let $Y_t=\#\left\{n‚©æ1:R_n‚©Ωt\right\},t‚©æ0$. Show that $Y_t$ is not a Poisson process with rate $Œª$, but does satisfy $‚Ñô\left(Y_t&lt;‚àû\right.$ for all $\left.t‚©æ0\right)=1$.<br>
[Hint: Let $B_n=1_{\left\{R_n‚©Ωt\right\}}$ and write $Y_t=\sum_{n‚©æ1}B_n$.]</li>
</ol></li></ol></div>
<ol type="a"><li>"‚áí": Let $x&lt;c$. Then $‚Ñô\left(Z_n‚â§x\right)‚â§‚Ñô\left(\left|Z_n-c\right|‚â•c-x\right)‚Üí0$. Let $x&gt;c$. Then $‚Ñô\left(Z_n‚â§x\right)=1-‚Ñô\left(Z_n&gt;x\right)‚â•1-‚Ñô\left(\left|Z_n-c\right|&gt;x-c\right)‚Üí1$. As the limit is the cdf of the constant $c$, which is discontinuous at $x=c$, convergence in distribution holds.<br>"‚áê": Let $Œµ&gt;0$. Then $‚Ñô\left(\left|Z_n-c\right|&gt;Œµ\right)‚â§‚Ñô\left(Z_n‚â§c-Œµ\right)+1-‚Ñô\left(Z_n‚â§c+Œµ\right)‚Üí0+1-1=0$. Hence convergence in probability holds.</li>
<li><ol type="i"><li>For all $t&lt;Œª$, the pdf $f_{r,Œª}$ integrates to 1, hence\begin{aligned} M_{X_{r}}(t)=ùîº\left[e^{t X_{r}}\right] &=‚à´_{0}^‚àûe^{t x} \frac{1}{Œì(r)} x^{r-1} Œª^{r} e^{-Œª x} d x \\ &=\frac{Œª^{r}}{(Œª-t)^{r}} ‚à´_{0}^‚àûf_{r, Œª-t}(x) d x=\left(\frac{Œª}{Œª-t}\right)^{r}\end{aligned}</li><li>For all $t&lt;rŒª$, we have$$M_{X_{r}/r}(t)=M_{X_{r}}(t/r)=\left(\frac{Œª}{Œª-\frac{t}{r}}\right)^{r}=\frac{1}{\left(1-\frac{t/Œª}{r}\right)^{r}}‚Üíe^{t/Œª}$$In particular, this holds for all $t‚àà‚Ñù$ and $r$ sufficiently large. Since the limit is the mgf of the constant $c=1/Œª$, the convergence theorem for mgfs yields $X_r/r‚Üíc=1/Œª$ in distribution, and by (a) also in probability. <i>Alternatively, convergence in probability can be established directly using Chebyshev's inequality, but require also the calculation of the mean and variance of $X_r$.</i>
</li></ol></li><li><ol type="i"><li>$\left(N_t,t‚â•0\right)$ is called $\operatorname{PP}(Œª)$ if (1) $N_0=0,(2)N_t-N_s‚àº\operatorname{Poi}((t-s)Œª)$ for all $0‚â§s&lt;t$, and (3) for all $n‚â•2$ and $0‚â§s_1&lt;t_1‚â§s_2&lt;t_2‚â§‚ãØ‚â§s_n&lt;t_n$, the variables $N_{t_j}-N_{s_j},1‚â§j‚â§n$, are independent.</li>
<li>$‚Ñô\left(T_1&gt;t\right)=‚Ñô\left(N_t-N_0=0\right)=e^{-Œª t}$ by (1)-(2), for all $t‚â•0$. Hence $T_1$ is exponentially distributed with parameter $Œª$.</li>
<li>Similarly, for all $t‚â•0$
$$
‚Ñô\left(T_n&gt;t\right)=‚Ñô\left(N_t‚â§n-1\right)=e^{-Œª t}\sum_{k=0}^{n-1}\frac{(Œª t)^k}{k !}.
$$
Differentiation yields $-f_{T_n}(t)$, hence
$$
f_{T_n}(t)=Œª e^{-Œª t}\sum_{j=0}^{n-1}\frac{(Œª t)^j}{j !}-e^{-Œª t}\sum_{k=1}^{n-1}\frac{Œª^k t^{k-1}}{(k-1)!}=e^{-Œª t}Œª^n t^{n-1}\frac{1}{(n-1)!}=f_{n,Œª}(t),
$$
the pdf of a Gamma distribution with shape parameter $n$ and rate parameter $Œª$.</li>
<li>First note that the first arrival time of $\left(Y_t,t‚â•0\right)$ is $\inf\left\{R_n,n‚â•1\right\}‚â§R_1$, and this is strictly smaller than $R_1$ with positive probability since $‚Ñô\left(R_2&lt;R_1\right)&gt;0$. Hence the first arrival time is not exponentially distributed with parameter $Œª$, so $\left(Y_t,t‚â•0\right)$ not a $\mathrm{PP}(Œª)$.<br>
Following the hint, we have for all $t‚â•0$
$$
ùîº\left[Y_t\right]=ùîº\left[\sum_{n‚â•1}B_n\right]=\sum_{n‚â•1}‚Ñô\left(R_n‚â§t\right)=\sum_{n‚â•1}‚Ñô\left(T_n‚â§t\right)=Œª t .
$$
In particular, $‚Ñô\left(Y_k&lt;‚àû\right)=1$ for all $k‚â•1$ and so $‚Ñô\left(Y_k&lt;‚àû\right.$ for all $\left.k‚â•1\right)=1$. Since $t‚Ü¶Y_t$ is (weakly) increasing, this already implies that on the same event $\left\{Y_k&lt;‚àû\right.$ for all $\left.k‚â•1\right\}$, also $Y_t&lt;‚àû$ for all $t‚â•0$
</li>
</ol></li></ol>
<div class="q">
    <ol type="a">
    <li>State the Central Limit Theorem.</li>
    <li>Let $(R,S)$ be a pair of random variables with joint probability density function
    $$
    f(r,s)=\begin{cases}\frac{1}{4}e^{-{|s|}},&(r,s)‚àà[-1,1]√ó‚Ñù\\0,&\text { otherwise. }\end{cases}
    $$
    Also consider independent identically distributed random variables $\left(R_n,S_n\right),n‚©æ1$, with the same joint distribution as $(R,S)$.<ol type="i"><li>Find the marginal probability density functions of $R$ and $S$.</li><li>For any $s‚àà‚Ñù$, determine
    $$
\lim _{n‚Üí‚àû}‚Ñô\left(\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_n‚©Ωs\right)
    $$</li><li>For any $r,s‚àà‚Ñù$, show that
    $$
\lim _{n‚Üí‚àû}‚Ñô\left(\frac{1}{\sqrt{n\operatorname{var}(R)}}\sum_{k=1}^n R_n‚©Ωr,\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_n‚©Ωs\right)=‚Ñô(W‚©Ωr,Z‚©Ωs)
    $$
    for a pair of random variables $(W,Z)$ whose joint distribution you should determine.</li></ol>
    </li><li>Consider the transformation $T:‚Ñù^2‚Üí‚Ñù^2$ given by $T(x,y)=(x-y,x+y)$. Let $(R,S)$ be as in (b) and $(X,Y)$ such that $(R,S)=T(X,Y)$.<ol type="i"><li>Derive the joint probability density function of $(X,Y)$.</li><li>Find the marginal probability density functions of $X$ and $Y$.</li><li>Find the correlation of $X$ and $Y$.</li></ol></li></ol>
</div>
<ol type="a"><li>If $V_n,n‚â•1$, are iid with mean $Œº$ and variance $œÉ^2‚àà(0,‚àû)$, for all $v‚àà‚Ñù$$$‚Ñô\left(\frac{1}{\sqrt{n œÉ^2}}\sum_{k=1}^n\left(V_n-Œº\right)‚â§v\right)‚Üí‚Ñô(Z‚â§v)‚ÄÉ\text{ as }n‚Üí‚àû$$where $Z‚àºN(0,1)$.</li>
<li><ol type="i">
<li>Joint pdf factorises, so read off $f_R(r)=\frac{1}{2},r‚àà[-1,1],f_S(s)=\frac{1}{2}e^{-{|s|}},s‚àà‚Ñù$.</li>
<li>Clearly $S_n,n‚â•1$, are iid with zero mean, by symmetry, and variance $\operatorname{var}(S)=ùîº\left[S^2\right]=2‚à´_0^‚àûs^2\frac{1}{2}e^{-s}d s=2‚àà(0,‚àû)$. Hence the CLT of (a) yields$$‚Ñô\left(\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_n‚â§s\right)‚Üí‚Ñô(Z‚â§s),\text{‚ÄÉfor all $s‚àà‚Ñù$, where $Z‚àºN(0,1)$.}$$</li><li>$R_n,n‚â•1$, are also iid with zero mean and $\operatorname{var}(R)=\frac{1}{3}‚àà(0,‚àû)$. As $f$ factorises, $R$ and $S$, and hence their partial sums are independent and by CLT
\begin{aligned}
& ‚Ñô\left(\frac{1}{\sqrt{n \operatorname{var}(R)}} \sum_{k=1}^n R_n‚â§r, \frac{1}{\sqrt{n \operatorname{var}(S)}} \sum_{k=1}^n S_n‚â§s\right) \\
&=‚Ñô\left(\frac{1}{\sqrt{n \operatorname{var}(R)}} \sum_{k=1}^n R_n‚â§r\right) ‚Ñô\left(\frac{1}{\sqrt{n \operatorname{var}(S)}} \sum_{k=1}^n S_n‚â§s\right) \\
&‚Üí‚Ñô(W‚â§r) ‚Ñô(Z‚â§s)=‚Ñô(W‚â§r, Z‚â§s)
\end{aligned}
where the joint distribution of $(W,Z)$ is determined by independence and marginal distributions $W,Z‚àºN(0,1)$</li></ol></li><li><ol type="i"><li>Linear transformation $T$, bijective with Jacobian determinant $J(x, y)=1 √ó 1-(-1) √ó 1=2$. By the transformation formula for pdfs, $(X, Y)$ has joint pdf
$$
f_{X, Y}(x, y)={|J(x, y)|} f_{R, S}(T(x, y))= \begin{cases}\frac{1}{2} e^{-{|x+y|}} & \text { if }{|x-y|} ‚â§ 1 \\ 0 & \text { otherwise. }\end{cases}
$$</li>
<li>By symmetry $X$ and $Y$ are identically distributed. Also, for ${|y|} ‚â• \frac{1}{2}$,
\begin{aligned}
f_Y(y)=f_Y({|y|}) & =‚à´_{|y|-1}^{|y|+1} \frac{1}{2} e^{-x-{|y|}} d x=\frac{1}{2} e^{-{|y|}}\left(e^{-{|y|}+1}-e^{-{|y|}-1}\right) \\
& =\frac{1}{2}\left(e-\frac{1}{e}\right) e^{-2{|y|}}
\end{aligned}
and for ${|y|} ‚â§ \frac{1}{2}$, <a href="http://kuing.infinityfreeapp.com/forum.php?mod=viewthread&tid=10759">we split into two integrals</a> over $({|y|}-1,-{|y|})$ and $(-{|y|},{|y|}+1)$ :
\begin{aligned}
f_Y(y)=f_Y({|y|}) & =\left(\frac{1}{2}-\frac{1}{2} e^{2{|y|}-1}\right)+\left(-\frac{1}{2} e^{-2{|y|}-1}+\frac{1}{2}\right) \\
& =1-\frac{1}{e} \cosh (2 y) .
\end{aligned}</li>
<li>$\operatorname{var}(Y)=\operatorname{var}(X)=\operatorname{var}\left(\frac{1}{2} S+\frac{1}{2} R\right)=\frac{1}{4} \operatorname{var}(S)+\frac{1}{4} \operatorname{var}(R)=\frac{7}{12}$ by independence. $\operatorname{cov}(X, Y)=\operatorname{cov}\left(\frac{1}{2}(S+R), \frac{1}{2}(S-R)\right)=\frac{1}{4}(\operatorname{var}(S)-\operatorname{var}(R))=\frac{5}{12}$, so $œÅ=\frac{5}{7}$.</li></ol></li></ol>
<div class="q">
<ol type="a"><li>Consider a Markov chain on a countable state space $S$ and let $i ‚àà S$.
<ol type="i"><li>Define the notions of recurrence and positive recurrence of $i$.
</li>
<li>Suppose that $i$ is positive recurrent. State, without proof, the ergodic theorem for the long-term proportion of time the Markov chain spends in state $i$.</li></ol>
</li>
<li>An urn contains a total of $N ‚©æ 2$ balls, some white and the others black. Each step consists of two parts. A first ball is chosen at random and removed. A second ball is then chosen at random from those remaining. It is returned to the urn along with a further ball of the same colour. Denote by $Y_n$ the number of white balls after $n ‚©æ 0$ steps.
<ol type="i"><li>Explain briefly why $\left(Y_n, n ‚©æ 0\right)$ is a Markov chain and determine its state space and transition matrix.
</li>
<li>Determine the communicating classes of this Markov chain and say whether their states are recurrent, and whether they are aperiodic. Justify your answers.
</li>
<li>Find all stationary distributions of this Markov chain.</li></ol>
</li>
<li>Now consider a Markov chain $\left(Z_n, n ‚©æ 0\right)$, on $I=\{0,1,2, ‚Ä¶, N\}$ with the transition matrix $P$ whose non-zero entries are
$$
p_{k, j}= \begin{cases}\frac{N-k}{N} \frac{k+1}{N+1} & \text { if } j=k+1, \\ \frac{N-k}{N} \frac{N-k}{N+1}+\frac{k}{N} \frac{k}{N+1} & \text { if } j=k, \\ \frac{k}{N} \frac{N-k+1}{N+1} & \text { if } j=k-1 .\end{cases}
$$
<ol type="i"><li>Show that the uniform distribution is stationary for this Markov chain. Hence, or otherwise, determine all stationary distributions of this Markov chain.
</li>
<li>For a state $k ‚àà I$, consider the successive visits
$$
V_1^{(k)}=\inf \left\{n ‚©æ 1: Z_n=k\right\} ‚ÄÉ \text { and } ‚ÄÉ V_{m+1}^{(k)}=\inf \left\{n ‚©æ V_m^{(k)}+1: Z_n=k\right\}, ‚ÄÉ m ‚©æ 1 .
$$
Explain why visits to $k$ occur in groups of independent geometrically distributed consecutive visits, and determine the parameter of this geometric distribution.
</li>
<li>Determine the expected time between two groups of visits to state $k$.
</li>
<li>Is the following statement true or false? "For any two states $k_1 ‚â† k_2$, there is, on average, one visit to $k_2$ between the first and second visits to $k_1$. " Provide a proof or counterexample.</li></ol></li></ol>
</div>
<ol type="a"><li>
<ol type="i"><li>Denote the MC by $\left(X_n\right)$. Let $H^{(i)}=\inf \left\{n ‚â• 1: X_n=i\right\}$. Then $i$ is recurrent, resp. positive recurrent, if $‚Ñô_i\left(H^{(i)}<‚àû\right)=1$, resp. $m_i:=ùîº\left[H^{(i)}\right]<‚àû$.
</li>
<li>In the given setting, for all $j ‚àà S$ with $‚Ñô_j\left(H^{(i)}<‚àû\right)=1$, we have
$$
‚Ñô_j\left(n^{-1} \#\left\{1 ‚â§ k ‚â§ n: X_k=i\right\} ‚Üí 1 / m_i\right)=1 .
$$</li></ol>
</li>
<li>S: (i)-(ii) standard, (iii) new but similar to other examples.
<ol type="i"><li>Only the numbers $Y_n$ of white and $N-Y_n$ of black balls in the urn at time $n$ are relevant for the future evolution of the urn, not how we got to this composition. The state space is $I=\{0,1,2, ‚Ä¶, N\}$ and the non-zero entries of the transition matrix are
$$
p_{k, j}= \begin{cases}\frac{N-k}{N} \frac{k}{N-1} & \text { if } j=k+1, \\ \frac{N-k}{N} \frac{N-k-1}{N-1}+\frac{k}{N} \frac{k-1}{N-1} & \text { if } j=k, \\ \frac{k}{N} \frac{N-k}{N-1} & \text { if } j=k-1 .\end{cases}
$$
</li>
<li>$\{0\}$ and $\{N\}$ are recurrent aperiodic singleton classes as $p_{0,0}=p_{N, N}=1$, while $\{1, ‚Ä¶, N-1\}$ is a transient class since $p_{i, i-1}=p_{i, i+1}>0$ for all $1 ‚â§ i ‚â§ N-1$. It is aperiodic since also $p_{i, i}>0$ for all $1 ‚â§ i ‚â§ N-1$.
</li>
<li>Stationary distributions vanish on transient classes. On the other hand, all distributions with this property, $(Œª, 0,0, ‚Ä¶, 0,1-Œª), 0 ‚â§ Œª ‚â§ 1$, are stationary.</li></ol>
</li>
<li>(i) is standard, (ii) is new, but an elementary observation, (iii) is similar to other questions where return times are split up. (iv) is new.
<ol type="i"><li>Let $œÄ_i=1 /(N+1), 0 ‚â§ i ‚â§ N$, be the uniform distribution on $I$. We need to check $œÄ P=œÄ$. Indeed, we have
\begin{aligned}
(œÄ P)_j & =œÄ_{j-1} p_{j-1, j}+œÄ_j p_{j, j}+œÄ_{j+1} p_{j+1, j} \\
& =\frac{1}{N+1}\left(\frac{N-(j-1)}{N} \frac{(j-1)+1}{N+1}+\frac{N-j}{N} \frac{N-j}{N+1}+\frac{j}{N} \frac{j}{N+1}+\frac{j+1}{N} \frac{N-(j+1)+1}{N+1}\right)=\frac{1}{N+1}=œÄ_j,
\end{aligned}
for all $0 ‚â§ j ‚â§ N$, with the conventions $p_{-1,0}=p_{N+1, N}=0$ that are appropriately included above. $œÄ$ is unique since this Markov chain is irreducible, by the same argument as in (b)(ii) here also including $p_{0,1}=p_{N, N-1}>0$.
</li>
<li>By the Markov property, the first step away from $k$ is like the first success in a sequence of Bernoulli trials with success probability $q_k:=1-p_{k, k}$.
</li>
<li>From (i) and lectures, the expected return time is $ùîº_k\left[V_1^{(k)}\right]=m_k=1 / œÄ_k=N+1$. Denote by $A$ the event that the first step is away from $k$. We want to find $x=ùîº_k\left[V_1^{(k)} ‚à£ A\right]$. Clearly $ùîº_k\left[V_1^{(k)} ‚à£ A^c\right]=1$. By the Law of Total Probability,
$$
N+1=m_k=ùîº_k\left[V_1^{(k)} ‚à£ A\right] ‚Ñô(A)+ùîº_k\left[V_1^{(k)} ‚à£ A^c\right] ‚Ñô\left(A^c\right)=x q_k+\left(1-q_k\right)
$$
Hence $x=\left(N+q_k\right) / q_k=1+N / q_k$.
</li>
<li>This statement is true. Assume there are $k_1$ and $k_2$ with $r ‚â† 1$ visits to $k_2$ between two consecutive visits to $k_1$, on average. By the strong Markov property, the numbers $W_k$ of visits are iid. Since $V_m^{\left(k_1\right)} ‚Üí ‚àû$ a.s., the ergodic theorem says the asymptotic proportion of time in $k_2$ up to $V_m^{\left(k_1\right)}$ is $1 /(N+1)$. But SLLN says
$$
\frac{W_1+‚ãØ+W_m}{V_m^{\left(k_1\right)}}=\frac{W_1+‚ãØ+W_m}{m} \frac{m}{V_m^{\left(k_1\right)}} ‚Üí r \frac{1}{m_{k_1}}=\frac{r}{N+1} ‚ÄÉ \text { a.s., as } m ‚Üí ‚àû .
$$
This contradicts the uniqueness of limits.</li></ol></li></ol>
