---
mathjax: true
tag: Probability
excerpt: sheet 1 â€” MT 2022
---
<style>div.noprint{border: solid 1px black;padding: 5px; width: max-content;max-width: 99%;}ol[type="a"]{padding-left: 20px;}ol[type="a"]>li{counter-increment: a;}ol[type="a"]>li::marker{content:"("counter(a,lower-alpha)") "}</style>$\DeclareMathOperator{\var}{Var}\newcommand{\abs}[1]{\left|#1\right|}$
<ol type="1"><li>A company sells lottery scratch-cards for Â£1 each. 1% of cards win the grand prize of Â£50, a further 20% win a small prize of Â£2, and the rest win no prize at all. Estimate how many cards the company needs to sell to be 99% sure of making an overall profit. $[Î¦(2.3263)=0.99]$<br>
    <b>Solution.</b><br>
    Let $X_i$ be the profit on a single ticket, $n$ be the number of cards, so the overall profit $S_n=X_1+â‹¯+X_n$.
    \begin{array}l
    â„™(X_i=x)=\begin{cases}79\%&x=1\\20\%&x=-1\\1\%&x=-49\\0&\text{else}\end{cases}\\
    ğ”¼X_i=1-1\%â‹…50-20\%â‹…2=0.1\\
    \var X_i=79\%(1-0.1)^2+1\%(-49-0.1)^2+20\%(-1-0.1)^2=24.99
    \end{array}
    By Central Limit Theorem, $\frac{S_n-0.1n}{\sqrt{24.99n}}âˆ¼N(0,1)$.
    $$â„™(S_n>0)=0.99â‡”\frac{0-0.1n}{\sqrt{24.99n}}=-2.3263â‡”n=13523.8$$
</li><li>A list consists of 1000 non-negative numbers. The sum of the entries is 9000 and the sum of the squares of the entries is 91000. Let $X$ represent an entry picked at random from the list. Find the mean of $X$, the mean of $X^2$, and the variance of $X$. Using Markov's inequality, show that the number of entries in the list greater than or equal to 50 is at most 180. What is the corresponding bound from applying Markov's inequality to the random variable $X^2$? What is the corresponding bound using Chebyshev's inequality?<br>
<b>Solution.</b><br>
$ğ”¼(X)=\frac{9000}{1000}=9,ğ”¼(X^2)=\frac{91000}{1000}=91,\var(X)=ğ”¼(X^2)-ğ”¼(X)^2=10$. By Markov's inequality $â„™(Xâ‰¥50)â‰¤\frac{ğ”¼(X)}{50}=0.18$, so $1000Ã—0.18=180$.<br>
$â„™(X^2â‰¥50^2)â‰¤\frac{ğ”¼(X^2)}{50^2}=0.0364$, so $1000Ã—0.0364=36$.<br>
By Chebyshev, $â„™(\left|X-ğ”¼(X)\right|>41)â‰¤\frac{\var(X)}{41^2}=0.006$, so $1000Ã—0.006=6$.
<div class="noprint"><a href="https://www.stat.berkeley.edu/~stark/SticiGui/Text/location.htm">https://www.stat.berkeley.edu/~stark/SticiGui/Text/location.htm</a></div>
</li><li>For $nâ‰¥1$, let $Y_n$ be uniform on $\{1,2,â€¦,n\}$ (i.e. taking each value with probability $1/n$). Draw the distribution function of $Y_n / n$. Show that the sequence $Y_n / n$ converges in distribution as $nâ†’âˆ$. What is the limit?
<center>
{% latex %}\usepackage{tikz}
\begin{tikzpicture}[scale=4]
\draw[<->,very thin](0,1.1)--(0,0)--(1.1,0);
\fill(0,.1)circle(.01)(.1,0)circle(.01)(0,1)circle(.01)(1,0)circle(.01);
\node at(.1,0)[below]{$\frac1n$};
\node at(0,.1)[left]{$\frac1n$};
\node at(1,0)[below]{1};
\node at(0,1)[left]{1};
\draw[red](1,1)--(1.1,1);
\draw[dashed,very thin](0,1)--(1,1)--(1,0);
\foreach\i in{0,.1,...,1}\draw[red](\i,\i)--(\i+.1,\i)node[circle,fill=white,draw,inner sep=1]{};
\end{tikzpicture}{% endlatex %}
</center>$$â„™\left(\frac{Y_n}nâ‰¤k\right)=\begin{cases}0&k< 0\\\frac{âŒŠnkâŒ‹}n&0â‰¤kâ‰¤1\\1&k>1\end{cases}$$
$$k-\frac1n<\frac{âŒŠnkâŒ‹}nâ‰¤k\text{ so the limit is }\begin{cases}0&k< 0\\k&0â‰¤kâ‰¤1\\1&k>1\end{cases}$$
</li><li>Let $X_i, iâ‰¥1$, be i.i.d. uniform on $[0,1]$. Let $M_n=\max\left\{X_1,â€¦, X_n\right\}$.
<ol type="a">
<li>Show that $M_nâ†’1$ in probability as $nâ†’âˆ$.</li>
<li>Show that $n\left(1-M_n\right)$ converges in distribution as $nâ†’âˆ$. What is the limit?</li>
</ol>
<b>Proof.</b>
<ol type="a">
<li>$â„™\left(\left|M_n-1\right|>Ïµ\right)=â„™\left(M_nâ‰¤1-Ïµ\right)=(1-Ïµ)^nâ†’0$ as $nâ†’âˆ$.</li>
<li>$â„™\left(n(1-M_n)â‰¤k\right)=1-â„™\left(M_n< 1-\frac kn\right)=\begin{cases}1&k>n\\1-\left(1-\frac kn\right)^n&0â‰¤kâ‰¤n\\0&k< 0\end{cases}â€ƒ$ converges to $â€ƒ\begin{cases}1-e^{-k}&kâ‰¥0\\0&k< 0\end{cases}â€ƒ$ so $n\left(1-M_n\right)\overset dâ†’\operatorname{Exp}(1)$.
<center>
{% latex %}\usepackage{tikz}
\draw(0,-0.15)node[below]{0}--(0,0.15)(0,0)--(4,0)(4,0.15)--(4,-0.15)node[below]{1};
\draw[<->](0.6,0.25)--(1.4,0.25);
\draw[<->](2.2,0.25)--(2.9,0.25);
\draw[<->](3.2,0.25)--node[above]{\footnotesize$1-M_n$}(4,0.25);
\def\cross{\tikz[scale=0.1]{\draw(-1,-1)--(1,1)(-1,1)--(1,-1);}}
\foreach\t in{0.2,0.6,1.4,2.2,2.9,3.2}\node at(\t,0){\cross};
\draw node at(0,0)[left]{\footnotesize$n$ many uniform points};
\draw[-stealth](-2,-1)--node[below]{\footnotesize$n\to\infty$}node[above]{\footnotesize Scale $n\times$}(-0.5,-1);
\draw[->](0,-1.15)node[below]{0}--+(0,0.3)(0,-1)--(5.2,-1)node[right]{$\infty$};
\foreach\t in{0.4,1,1.7,2.7,3.3,4.2,4.6}\node at(\t,-1){\cross};
\draw[<->](1.7,-1.25)--node[below]{\footnotesize Exp(1)}(2.7,-1.25);
\draw[<->](3.3,-1.25)--node[below]{\footnotesize Exp(1)}(4.2,-1.25);
\end{tikzpicture}{% endlatex %}
</center>
<div class="noprint"><a href="https://en.wikipedia.org/wiki/Poisson_point_process#Poisson_distribution_of_point_counts">Poisson point process</a><br>
    <a href="https://math.stackexchange.com/questions/3231470">Transforming sum of n exponential distribution to a Poisson distribution</a></div>
</li>
</ol>
</li><li><ol type="a">
<li>
    What is the distribution of the sum of $n$ independent Poisson random variables each of mean 1? Use the central limit theorem to deduce that
    \[
    e^{-n}\left(1+n+\frac{n^2}{2 !}+â‹¯+\frac{n^n}{n !}\right)â†’\frac12\text{ as } nâ†’âˆ .
    \]
</li>
<li>
    Let $pâˆˆ(0,1)$. What is the distribution of the sum of $n$ independent Bernoulli random variables with parameter $p$? Let $0â‰¤a< bâ‰¤1$. Use appropriate limit theorems to determine how the value of
    \[
    \lim _{nâ†’âˆ} \sum_{râˆˆâ„•:anâ‰¤r< bn}\binom nr p^r(1-p)^{n-r}
    \]
    depends on $a$ and $b$.
</li></ol>
<b>Proof.</b><ol type="a">
    <li>
        Let $X_1,â€¦,X_n\overset{\text{i.i.d}}âˆ¼\operatorname{Po}(1)$, then $S_n=âˆ‘_{i=1}^nX_iâˆ¼\operatorname{Po}(n)$.<br>
        By Central Limit Theorem, $e^{-n}\left(1+n+\frac{n^2}{2!}+â‹¯+\frac{n^n}{n!}\right)=â„™(S_nâ‰¤n)=â„™\left(\frac{S_n}n-1â‰¤0\right)â†’Î¦(0)=\frac12$ as $nâ†’âˆ$.
    </li>
    <li>We proved some parts of this question by Weak Law of Large Numbers in <a href="https://cjhb.site/index.php?dir=116%2F8.html">Prelims</a>:<br>When $pâˆˆ[a,b]$, there exists $Ïµ$ such that $[p-Ïµ,p+Ïµ]âŠ‚[a,b]$, so $â„™(anâ‰¤r< bn)â‰¥â„™(\abs{r-pn}â‰¤Ïµ)â†’1$ as $nâ†’âˆ$.<br>When $pâˆˆ[0,n]âˆ–[a,b]$, there exists $Ïµ$ such that $[p-Ïµ,p+Ïµ]âŠ‚[0,n]âˆ–[a,b]$, so $â„™(anâ‰¤r< bn)â‰¤â„™(\abs{r-pn}>Ïµ)â†’0$ as $nâ†’âˆ$.<br>The cases not included in Prelims is $a< b=p$ or $a=p< b$. For the first case: (can do the second case similarly)<br>
    $X_i\overset{\text{i.i.d}}âˆ¼\operatorname{Ber}(p)$, then $S_n=\sum_{i=1}^n X_iâˆ¼\operatorname{Bin}(n,p)$, so
    \begin{align*}\sum_{râˆˆâ„•:pnâ‰¤râ‰¤bn}\binom nr p^r(1-p)^{n-r}&=â„™\left(pnâ‰¤\operatorname{Bin}(n,p)â‰¤bn\right)\\
    &=â„™\left(pnâ‰¤S_nâ‰¤bn\right)\\
    &=â„™\left(0â‰¤\frac{S_n}n-pâ‰¤b-p\right)\\
    &=\frac12â„™\left(\abs{\frac{S_n}n-p}â‰¤b-p\right)\\
    &â†’\frac12â‹…1=\frac12
    \end{align*}
    Summing up all the cases, $\limâ„™\left(anâ‰¤râ‰¤bn\right)=\cases{0&if $a< b< p$ or $p< a< b$\\1/2&if $a< b=p$ or $a=p< b$\\1&if $a< p< b$}$
</li>
</ol>
</li><li><ol type="a">
<li>Let $X_n, nâ‰¥1$, be a sequence of random variables defined on the same probability space. Show that if $X_nâ†’c$ in distribution, where $c$ is a constant, then also $X_nâ†’c$ in probability.</li>
<li>Show that if $ğ”¼\left|X_n-X\right|â†’0$ as $nâ†’âˆ$, then $X_nâ†’X$ in probability. Is the converse true?</li>
</ol>
<b>Proof.</b>
<ol type="a">
<li>Since $X_n\overset dâ†’c$ and $F_c$ is continuous on $ â„âˆ–\{c\}$ we have $âˆ€Ïµ>0,\lim_{nâ†’âˆ}F_{X_n}(c-Ïµ)=F_c(c-Ïµ)=0,\lim_{nâ†’âˆ}F_{X_n}\left(c+Ïµ\right)=F_c\left(c+Ïµ\right)=1$.
\begin{align*}
\lim_{nâ†’âˆ} â„™\left(\abs{X_n-c}â‰¥Ïµ\right) &=\lim_{nâ†’âˆ} â„™\left(X_nâ‰¤c-Ïµ\right) + \lim_{nâ†’âˆ}â„™\left(X_nâ‰¥c+Ïµ\right)\\
&=\lim_{nâ†’âˆ} F_{X_n}(c-Ïµ)+1-\lim_{nâ†’âˆ}F_{X_n}(c+Ïµ)\\
&=0\implies X_n\overset Pâ†’c
\end{align*}
{% latex %}\usepackage{tikz}\usepackage{amsfonts}
\begin{tikzpicture}[every node/.append style={align=left}]
\node(Lp)at(-2,1.5){$L^p$ convergence\\$\mathbb E[|X_n-X|^p]^{1/p}\to0$};
\node(L1)at(2,1.5){$L^1$ convergence\\$\mathbb E[|X_n-X|]\to0$};
\draw[-stealth](Lp)--(L1);
\node(as)at(-2,-1.5){almost sure\\convergence};
\node(pr)at(2,-1.5){convergence\\in probability};
\node(di)at(5.5,-1.5){convergence\\in distribution};
\draw[-stealth](as)--(pr);
\draw[-stealth](pr)--(di);
\draw[-stealth](1.5,1)--(1.5,-1);
\draw[stealth-,dashed](2.5,1)--node[right]{uniform\\integrability}(2.5,-1);
\draw[-stealth,thick,dashed](5.25,-2.0311) arc(-60:-120:3.5);
\node at (3.5,-2.25) {$X_n\to c$};
\end{tikzpicture}{% endlatex %}
</li>
<li>$âˆ€Ïµ>0,âˆƒN,âˆ€n>N,ğ”¼\abs{X_n-X}< Ïµ^2$, by Markov's inequality, $â„™(\abs{X_n-X}â‰¥Ïµ)â‰¤\frac{ğ”¼\abs{X_n-X}}Ïµ< Ïµ$, so $X_n\overset{P}â†’X$.<br>
The converse is false. $â„™(X_n=n)=\frac1n,â„™(X_n=0)=\frac{n-1}n$, then $X_n\overset{P}â†’0$ but $ğ”¼X_nâ†’1â‰ 0$.
</li>
</ol>
</li><li>A gambler makes a long sequence of bets against a rich friend. The gambler has initial capital $C$. On each round, a coin is tossed; if the coin comes up tails, he loses 30% of his current capital, but if the coin comes up heads, he instead wins 35% of his current capital.
<ol type="a">
<li>Let $C_n$ be the gambler's capital after $n$ rounds. Write $C_n$ as a product $CY_1Y_2â€¦Y_n$ where $Y_i$ are i.i.d. random variables. Find $ğ”¼C_n$.</li>
<li>Find the median of the distribution of $C_{10}$ and compare it to $ğ”¼C_{10}$.</li>
<li>Consider $\log C_n$. What does the law of large numbers tell us about the behaviour of $C_n$ as $nâ†’âˆ$ ? How is this consistent with the behaviour of $ğ”¼C_n$ ?</li>
</ol>
<b>Solution.</b><br>
<ol type="a">
<li>$C_n=CY_1Y_2â€¦Y_n$ where $Y_i$ is defined by $â„™(Y_i=0.7)=â„™(Y_i=1.35)=\frac12$.<br>
$ğ”¼Y_i=0.7Ã—1/2+1.35Ã—1/2=1.025$ and $Y_i$ are independent, so $ğ”¼C_n=ğ”¼Câ‹…ğ”¼Y_1â‹…ğ”¼Y_2â‹…â‹¯â‹…ğ”¼Y_n=1.025^nC$.</li>
<li>The median of $C_n$ is $Câ‹…0.7^5â‹…1.35^5=0.753631C$, less than $ğ”¼C_{10}=1.28008C$.</li>
<li>$\log C_n=\log C+âˆ‘_{i=1}^n\log Y_i$. By the law of large numbers, $\frac{âˆ‘_{i=1}^n\log Y_i}nâ†’ğ”¼[\log Y_i]=(\log0.7+\log1.35)Ã—1/2=\log0.972$, so $\log C_nâ†’\log C+n\log0.972$, so $\exp ğ”¼[\log C_n]=0.972^nC$.<br>
This is less than $ğ”¼[C_n]$ obtained in (a) by Jensen's inequality (because log is concave function, $ğ”¼[\log X]â‰¤\log ğ”¼[X]$).<br>
To prove $C_nâ†’0$ rigorously, by Strong Law of Large numbers $\frac{âˆ‘_{i=1}^n\log Y_i}n\overset{a.s.}âŸ¶\log0.972$, so $âˆƒN,âˆ€n>N:\frac{âˆ‘_{i=1}^n\log Y_i}nâ‰¤\log0.99â‡’C_n< C0.99^nâ†’0$ as $nâ†’âˆ$. Applying the same argument to $ğ”¼[C_n]$ is useless: we can only get $C_n$ less than âˆ.
<div class="noprint"><a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox#Expected_utility_theory">Expected utility theory</a><br><a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly criterion</a></div>
</li></ol>
</li><li>Let $â„_n$ be the $n$-dimensional cube $[-1,1]^n$. For fixed $x \in â„$, show that the proportion of the volume of $â„_n$ within distance $(n/3)^{1/2}+x$ of the origin converges as $nâ†’âˆ$, and find the limit.<br>
[<i>Hint: Consider a random point whose $n$ coordinates are i.i.d. with Uniform $[-1,1]$ distribution. If $AâŠ‚â„_n$, then $\operatorname{vol}(A) / \operatorname{vol}\left(â„_n\right)$ is the probability that such a point falls in the set $A$. Let $D_n$ represent the distance of such a point from the origin; apply an appropriate limit theorem to $D_n^2$.</i>]<br>
<b>Solution.</b><br>
Take a random point $(X_1,X_2,â€¦,X_n)âˆˆ[0,1]^n$. Its distance to the origin is $\sqrt{âˆ‘X_i^2}$. We want to compute probability of $S_n=âˆ‘X_i^2â‰¤\big((n/3)^{1/2}+x\big)^2$.
\begin{align*}
ğ”¼(X_i^2)&=\int_0^1x^2\mathrm{d}x=\frac13\\
ğ”¼(X_i^4)&=\int_0^1x^4\mathrm{d}x=\frac15\\
â‡’\operatorname{Var}(X_i^2)&=ğ”¼(X_i^4)-ğ”¼(X_i^2)^2=\frac4{45}
\end{align*}
Apply the central limit theorem to $X_i^2$, as $nâ†’âˆ$,
$$\frac{S_n-n/3}{\sqrt{4n/45}}\overset dâ†’N(0,1)$$
\begin{align*}
â„™\Big(S_nâ‰¤\big((n/3)^{1/2}+x\big)^2\Big)
&= â„™\left(\frac{S_n-n/3}{\sqrt{4n/45}}â‰¤\frac{\big((n/3)^{1/2}+x\big)^2-n/3}{\sqrt{4n/45}}\right)
\\&= â„™\left(\frac{S_n-n/3}{\sqrt{4n/45}}â‰¤\sqrt{15}x+\frac{3\sqrt{5}}{2\sqrt{n}}x^2\right)\\&â†’â„™\left(Zâ‰¤\sqrt{15}x\right)=Î¦\left(\sqrt{15}x\right)
\end{align*}
<div class="noprint"><a href="https://math.stackexchange.com/questions/2111920">Showing proportion of volume of n-dimensional cube within distance ${n/3}^{1/2} + x$ converges as $nâ†’âˆ$</a></div>
</li><li>Let $Y_1, Y_2, â€¦$ be i.i.d. and uniformly distributed on the set $\{1,2,â€¦, n\}$. Define $X^{(n)}=\min \left\{kâ‰¥1: Y_k=Y_j\text{ for some }j< k\right\}$, the first time that we see a repetition in the sequence $Y_i$. Interpret the case $n=365$. Prove that $X^{(n)}/\sqrt{n}$ converges in distribution to a limit with distribution function $F(x)=1-\exp \left(-x^2 / 2\right)$ for $x>0$.<br>
<b>Proof.</b><br>
Let $m=âŒŠk\sqrt nâŒ‹-1$, then $X^{(n)}/\sqrt n>kâ‡”X^{(n)}>m+1â‡”Y_2âˆ‰\{Y_1\}âˆ§Y_3âˆ‰\{Y_1,Y_2\}âˆ§â‹¯âˆ§Y_{m+1}âˆ‰\{Y_1,Y_2, â€¦,Y_m\}$,
\begin{align*}
â„™\left(\frac{X^{(n)}}{\sqrt n}>k\right)&=\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) â‹¯\left(1-\frac{m}{n}\right)\\
\logâ„™\left(\frac{X^{(n)}}{\sqrt n}>k\right)&=\sum_{i=1}^m\log\left(1-\frac{i}{n}\right)
\end{align*}
Using $-h-h^2<\log (1-h)<-h$ for $hâˆˆ(0,1/2)$,
\[-S_n-T_n<\logâ„™\left(\frac{X^{(n)}}{\sqrt n}>k\right)<-S_n\]
where\[S_n=\frac1n\sum_{i=1}^miâ€ƒT_n=\frac1{n^2}\sum_{i=1}^mi^2\]
As $nâ†’âˆ$,\[S_nâˆ¼\frac{m^2}{2n}â€ƒT_nâˆ¼\frac{m^3}{3n^2}\]
Using $mâˆ¼k\sqrt n$,\[S_nâˆ¼\frac{k^2}2â€ƒT_nâˆ¼\frac{k^3}{3\sqrt n}â†’0\]
Then $â„™\left(X^{(n)}/\sqrt n>k\right)â†’\operatorname{Exp}(k^2/2)$. Hence $X^{(n)}/\sqrt n\overset dâ†’X$. The case $n=365$ is the birthday problem.
<div class="noprint">
<a href="https://math.stackexchange.com/questions/990808">Generalization of the birthday problem with convergence in distribution</a><br>
<a href="https://math.stackexchange.com/questions/2113354">Convergence in distribution to a limit with distribution function $F(x) = 1-\exp(-x^2/2)$ for $x>0$</a><br>
<a href="https://math.stackexchange.com/questions/1835804">I am trying to prove the distribution function for the 'birthday problem' can anyone help?</a>
</div>
</li><li>Let $X_i, iâ‰¥1$, be i.i.d. random variables with $â„™\left(X_i=0\right)=â„™\left(X_i=1\right)=1/2$.<ol type="a">
<li>Define $S_n=\sum_{i=1}^n X_i 2^{-i}$. What is the distribution of $S_n$? Show that the sequence $S_n$ converges almost surely as $nâ†’âˆ$. What is the distribution of the limit $S$?</li>
<li>
    Define $R_n=\sum_{i=1}^n 2 X_i 3^{-i}$. Show again that the sequence $R_n$ converges almost surely. Is the limit a discrete random variable? Is it a continuous random variable?
    [<i>Hint: Consider its expansion in base 3.</i>]
</li>
</ol>
<b>Solution.</b>
<ol type="a">
<li>$S_n$ has binary expansion $0\mathbin.X_1â‹¯X_n$, so $S_nâˆ¼$discrete uniform distribution on $\{2^{-n}i:i=0,1,â‹¯,2^n-1\}$.<br>$\abs{S_n-S_{n+m}}=âˆ‘_{i=n+1}^{n+m}X_i2^{-i}<âˆ‘_{i=n+1}^âˆ2^{-i}=2^{-n}$, so $(S_n)_{nâ‰¥1}$ is a Cauchy sequence, so $S_n\overset{a.s.}âŸ¶Sâˆ¼U[0,1]$.</li>
<li>$\abs{R_n-R_{n+m}}=âˆ‘_{i=n+1}^{n+m}2X_i3^{-i}<âˆ‘_{i=n+1}^âˆ2â‹…3^{-i}=3^{-n}$, so $(R_n)_{nâ‰¥1}$ is a Cauchy sequence, so $R_n$ converges almost surely.<br>
Suppose $X_1,â‹¯,X_{k-1}$ are fixed. Let $H=\sum_{i=1}^{k-1}2 X_i 3^{-i}$.<br>
If $X_k=0$, we have$$R_âˆ=H+\sum_{i=k+1}^âˆ 2 X_i 3^{-i}â‰¤H+\sum_{i=k+1}^âˆ 2â‹…1â‹…3^{-i}=H+3^{-k}$$
If $X_k=1$, we have$$R_âˆ=H+2â‹…3^{-k}+\sum_{i=k+1}^âˆ 2 X_i 3^{-i}â‰¥H+2â‹…3^{-k}+\sum_{i=k+1}^âˆ 2â‹…0â‹…3^{-i}=H+2â‹…3^{-k}$$
So $R_âˆâˆ‰(H+3^{-k},H+2â‹…3^{-k})$. Let $F$ be the distribution function of $R_âˆ$. Then $F$ is constant on those intervals.<br>
$F$ is increasing and onto (any number in $[0,1]$ has a binary expansion), so it is continuous.<br>
$R_âˆ$ is neither a continuous random variable nor a discrete random variable.
<div class="noprint">
<a href="https://math.stackexchange.com/questions/984503">Convergence of Bernoulli distributed random variables with parameter $1/2$?</a><br>
Wikipedia â€“ <a href="https://en.wikipedia.org/wiki/Cantor_distribution">Cantor distribution</a><br>
<a href="https://cjhb.site/index.php?dir=264%2F1.html">Notes on Uniform Continuity</a><br>
<a href="http://kuing.infinityfreeapp.com/forum.php?mod=viewthread&tid=8999">Cantorå‡½æ•°çš„è¿ç»­æ€§æ¨¡é‡$Ï‰(Î´)â‰¤Î´^{\log2/\log3}$</a>
</div>
</li>
</ol>
</li><li>Consider a random variable $U: Î©â†’â„$ that is defined on a probability space $(Î©,â„±,ğ’«)$ and that is uniformly distributed on $[0,1]$. For $nâ‰¥1$, let $B_n=1$ if $âŒŠ2^nUâŒ‹$ is an odd integer and $B_n=0$ otherwise.
<ol type="a">
<li>Show that $B_n, nâ‰¥1$, is a sequence of i.i.d. random variables with $â„™\left(B_n=0\right)=â„™\left(B_n=1\right)=1/2$.</li>
<li>Apply 10.(a) to suitable subsequences $\left(X_i\right)$ of $\left(B_n\right)$ to construct two independent random variables on $(Î©,â„±,ğ’«)$, each distributed like $S$.</li>
<li>
    Adapt your answer to (b) to construct a sequence of i.i.d. random variables on $(Î©,â„±,ğ’«)$, each distributed like $S$.<br>
    [<i>In Prelims Probability, and also in Part A Probability, the existence of probability spaces
    for even a single continuous random variable or for sequences of discrete random variables are assumed without proof. In Part A Integration, the Lebesgue Ïƒ-algebra and
    Lebesgue measure on â„ are constructed. By restricting them to $â„¦:=[0,1]$, we obtain
    a probability space $(â„¦,â„±,ğ’«)$, on which the random variable $U(Ï‰)=Ï‰$ is uniformly
    distributed on $[0,1]$. The present problem demonstrates that this probability space is,
    in fact, also suitable for sequences of i.i.d. discrete or continuous random variables.
    Problem 10 suggests an alternative approach to suitable probability spaces of the form
    $â„¦=\{0,1\}^â„•:=\{(x_i):x_iâˆˆ\{0,1\},iâˆˆâ„•\}$. A subtle point for both is that (assuming the
    Axiom of Choice), â„± cannot be chosen to be the entire power set of $â„¦$ and the definition/existence of the probability measure $â„™:â„±â†’[0,1]$ is not entirely straightforward.</i>]
</li>
</ol>
<b>Solution.</b>
</li><li>Let $A_n$ be the median of $2n+1$ i.i.d. random variables which are uniform on $[0,1]$. Find the probability density function of $A_n$. Deduce a convergence in distribution result for the median (appropriately rescaled) as $nâ†’âˆ$ (feel free to argue informally if you like!).<br>
[<i>Hint: consider the probability that $A_n$ lies in a small interval $(x,x+Ïµ)$. How does the density at the point $\left(\frac{1}{2}+\frac{a}{\sqrt{n}}\right)$ behave as $nâ†’âˆ$ ? (Stirling's formula may be useful).</i>]<br>
</li></ol>
<a href="https://math.stackexchange.com/questions/236581">Convergence in distribution (weak convergence)</a><br>
<a href="https://math.stackexchange.com/questions/402648">Let $f$ be a bounded uniformly continuous function in $R^1$. Then $X_n\to 0$ in pr. implies $E(f(X_n)) \to f(0)$.</a><br>
<a href="https://math.stackexchange.com/questions/789282">Weak convergence: equivalence of definitions</a><br>
<a href="https://math.stackexchange.com/questions/4145588">convergence in distribution from convergence of norm</a><br>
<a href="https://math.stackexchange.com/questions/4139501">Prove that $\lim_{m \to \infty} (\lceil m U \rceil -mU ) \sim U(0,1)$</a><br>