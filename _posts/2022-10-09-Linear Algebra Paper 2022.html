---
mathjax: true
tag: Linear Algebra
excerpt: A0 paper 2022
---
$\DeclareMathOperator{\im}{Im}\DeclareMathOperator{\re}{Re}\DeclareMathOperator{\span}{Span}\DeclareMathOperator{\diag}{diag}\DeclareMathOperator{\tr}{tr}$<ol>
<li>Let $W â©½ V$ be finite-dimensional complex vector spaces and let $T: V â†’ V$ be a linear map.
<ol type="a"><li>Let $â„¬$ be a basis for $V$ containing a basis $â„¬_1$ for $W$, and let $â„¬_2:=â„¬âˆ–â„¬_1$.
<ol type="i"><li>Show that $q\left(â„¬_2\right)$ is a basis for $V / W$, where $q: V â†’ V / W$ is given by $q(v)=v+W$.
</li>
<li>Suppose that $W$ is $T$-invariant. Prove that there exists a well-defined linear map $\bar{T}: V / W â†’ V / W$ such that $\bar{T} âˆ˜ q=q âˆ˜ T$.
</li>
<li>Show that there exists a matrix $C$ such that$$\sideset{_â„¬}{_â„¬}{[T]}={\mmlToken{mo}(\begin{smallmatrix}\hskip5mu[{T|}_W]_{â„¬_1\kern-18.5mmâ„¬_1}&C\\0&\sideset{_{q(â„¬_2)}}{_{q(â„¬_2)}}{[\bar T]}\end{smallmatrix}\mmlToken{mo})}$$
</li>
<li>State and prove the Cayley-Hamilton Theorem.</li></ol>
</li>
<li>Now suppose that $V$ has basis $\{u, v\}$ and that $T(u)=i u$ and $T(v)=u+i v$.
<ol type="i"><li>Prove that $\{u, i u, v, i v\}$ is a basis for $V$, viewed as an $â„$-vector space.
</li>
<li>Calculate the matrix $A$ of the $â„$-linear map $T: V â†’ V$ with respect to $\{u, i u, v, i v\}$.
</li>
<li>Find the minimal polynomial of $A$, justifying your answer.
</li>
<li>Calculate the Jordan Normal Form of $A$, justifying your answer.</li></ol></li></ol>
</li>
<li>Let $V$ be a finite dimensional vector space over a field $ğ”½$, and let $T: V â†’ V$ be a linear map.
<ol type="a"><li>Let $p(x), q(x) âˆˆ ğ”½[x]$ be coprime polynomials such that $\im p(T) âŠ† \ker q(T)$.
<ol type="i"><li>Prove that $V=\ker p(T) âŠ• \ker q(T)$.
</li>
<li>Is the conclusion of (a)(i) true if $p(x)$ and $q(x)$ are not necessarily coprime?
</li>
<li>Suppose that the minimal polynomial of $T$ is equal to a product of distinct linear factors over $ğ”½$. Prove that $T$ is diagonalisable.
</li>
<li>Does the converse to (a)(iii) hold?</li></ol>
</li>
<li>Suppose that $T^p=I$ for some prime number $p$. Assume that the characteristic of $ğ”½$ is zero, and that there exists $Î¶ âˆˆ ğ”½$ such that $Î¶^p=1$ but $Î¶ â‰  1$. Let $â„°$ denote the set of all $ğ”½$-linear maps $S: V â†’ V$, and for each integer $i$ let
$$
â„°(i):=\left\{S âˆˆ â„°: T S=Î¶^i S T\right\}
$$
<ol type="i"><li>Using part (a)(iii) applied to a suitable map defined on $â„°$, prove that
$$
â„°=â„°(0) âŠ• â„°(1) âŠ• â‹¯ âŠ• â„°(p-1)
$$
For each $S âˆˆ â„°$ and any integer $i$, let
$$
Ïµ_i(S):=\sum_{j=0}^{p-1} Î¶^{i j} T^j S T^{p-j} âˆˆ â„° .
$$
</li>
<li>Show that $Ïµ_i(S) âˆˆ â„°(-i)$ for any integer $i$.
</li>
<li>Deduce that $Ïµ_i(S)^p$ commutes with $T$ for any integer $i$.
</li>
<li>Show that if $S â‰  0$, then $Ïµ_i(S) â‰  0$ for at least one integer $i$.
[Hint: you may find it helpful to consider the sum $Ïµ_0(S)+â‹¯+Ïµ_{p-1}(S)$.]</li></ol></li></ol>
</li>
<li>
<ol type="a"><li>
<ol type="i"><li>Define what is meant by a <i>complex inner product space</i>.
</li>
<li>Let $M_n(â„‚)$ be the vector space of $n Ã— n$ complex matrices. Prove that
$$
âŸ¨ x, yâŸ©:=\operatorname{tr}\left(\bar{x} y^T\right) â€ƒ \text { for } x, y âˆˆ M_n(â„‚)
$$
turns $M_n(â„‚)$ into a complex inner product space.
</li>
<li>Let $X:=\left\{x âˆˆ M_n(â„‚): x_{i j}=0\right.$ for all $\left.i>j\right\}$ be the subspace of upper triangular matrices. Find the orthogonal complement $X^{âŸ‚}$ of $X$ in $M_n(â„‚)$.</li></ol>
</li>
<li>Let $U, V$ and $W$ be finite dimensional complex inner product spaces.
<ol type="i"><li>Let $Î±: U â†’ V$ be a linear map. Define the adjoint map $Î±^*: V â†’ U$, and prove that it is unique. Show that $\left(Î±^*\right)^*=Î±$.
[You may assume that $Î±^*: V â†’ U$ always exists.]
</li>
<li>Show that $\imÎ±âˆ©\kerÎ±^*=\{0\}$.
</li>
<li>Let $Î²: V â†’ W$ be another linear map such that $\imÎ±=\kerÎ²$, and let
$$
Î³:=Î± Î±^*+Î²^* Î²
$$
Prove that $Î³: V â†’ V$ is invertible. <a href="https://math.stackexchange.com/questions/3821063">MSE</a></li></ol></li></ol>
</li>
</ol>
<h1>Solution</h1>
<ol><li><ol type="a"><li><ol type="i"><li>Let $\{e_1, â‹¯, e_k\}=â„¬_1,\{e_{k+1}, â‹¯, e_n\}=â„¬_2$.<br>To show $V/W=\spanâ„¬_2$. Take any $q(v)âˆˆV/W$, since $V=\spanâ„¬$, $âˆƒa_1, â‹¯, a_n âˆˆâ„‚:$
$$
v=a_1 e_1+â‹¯+a_k e_k+a_{k+1} e_{k+1}+â‹¯+a_n e_n
$$Since $e_1,â€¦,e_kâˆˆ\ker q$,
\[q(v)=a_{k+1}\left(q(e_{k+1})\right)+â‹¯+a_n\left(q(e_n)\right)\]
To show linear independence, assume for some $a_{k+1},â€¦,a_nâˆˆâ„‚:$
$$
a_{k+1}\left(q(e_{k+1})\right)+â‹¯+a_n\left(q(e_n)\right)=0_W
$$
Then $a_{k+1}e_{k+1}+â‹¯+a_ne_nâˆˆ\ker q=W$. Then $âˆƒa_1,â€¦,a_kâˆˆâ„‚:$
$$
a_1 e_1+â‹¯+a_k e_k=a_{k+1} e_{k+1}+â‹¯+a_n e_n
$$Since â„¬ is linearly independent, $a_{k+1}=â‹¯=a_n=0$.</li>
<li>For any $q(v)âˆˆV/W$, define $\bar{T}(q(v))=q(T(v))$.<br>
For any $wâˆˆW$, $T(w)âˆˆW$, so $\bar{T}(q(w))=q(T(w))=0$. $\bar T$ is well-defined <a href="https://courses.maths.ox.ac.uk/pluginfile.php/30064/mod_resource/content/2/A0.pdf#page=15&zoom=auto,-78,139">Lemma 3.8</a></li>
<li>Let $a_{ij}âˆˆâ„‚$ be the $(i,j)$-entry of $\sideset{_â„¬}{_â„¬}{[T]}$<br>For $jâ‰¤k$, $e_jâˆˆW$, so $T(e_j)âˆˆW$<br>
â€ƒso for $i>k,a_{ij}=0$; for $iâ‰¤k$, $a_{ij}$ is equal to the $(i,j)$-entry of $\sideset{_{â„¬_1}}{_{â„¬_1}}{[{T|}_W]}$
<br>For $j>k$, $q(T(e_j))=\bar T(q(e_j))$,<br>â€ƒso for $i>k$, $a_{ij}$ is equal to the $(i-k,j)$ entry of $\sideset{_{q(â„¬_2)}}{_{q(â„¬_2)}}{[\bar T]}$
</li><li>If $V$ is a finite-dimensional â„‚-vector space and $T:Vâ†’V$ is linear, then $Ï‡_T(T)=0$.<br>
Proof: $âˆƒPâˆˆM_n(â„‚):A=P^{-1}TP$ is upper triangular. Suppose the theorem holds for $A$,\[Ï‡_T(x)=\det(xI-T)=\det\left(P(xI-A)P^{-1}\right)=\det(xI-A)=Ï‡_A(x)\]$â‡’Ï‡_T(T)=Ï‡_A(T)=PÏ‡_A(A)P^{-1}=0$, the theorem holds for $T$.<br>It remains to prove the theorem for any upper triangular matrix $A=\pmatrix{Î»_1&*&*\\&â‹±&*\\&&Î»_n}$<br>
Let $e_1,â€¦,e_n$ be the standard basis vectors for $â„‚^n$. Then for all $vâˆˆâ„‚^n$\begin{align*}(A-Î»_nI)v&âˆˆâŸ¨e_1,â€¦,e_{n-1}âŸ©\\
(A-Î»_{n-1}I)(A-Î»_nI)v&âˆˆâŸ¨e_1,â€¦,e_{n-2}âŸ©\\
&â‹®\\
(A-Î»_1I)â‹¯(A-Î»_{n-1}I)(A-Î»_nI)v&âˆˆ\{0\}
\end{align*}so $Ï‡_A(A)=(A-Î»_1I)â‹¯(A-Î»_{n-1}I)(A-Î»_nI)=0$ as required.
</li></ol></li><li><ol type="i"><li>spanning: $âˆ€xâˆˆV,âˆƒÎ»,Î¼âˆˆâ„‚:x=Î»u+Î¼v$<br>
$â‡’x=(\reÎ»)u+(\imÎ»)iu+(\reÎ¼)v+(\imÎ¼)ivâˆˆâŸ¨u,iu,v,ivâŸ©_â„$.<br>
linear independent: $a,b,c,dâˆˆâ„,au+biu+cv+div=0â‡’(a+bi)u+(c+di)v=0$<br>$\{u,v\}$ are linearly independent over â„‚$â‡’a+bi=c+di=0â‡’a=b=c=d=0$.</li>
<li>$T(u)=iu,T(iu)=-u,T(v)=u+iv,T(iv)=iu-vâ‡’A=\pmatrix{&-1&1\\
1&&&1\\
&&&-1\\
&&1}$</li><li>By Cayley-Hamilton $m_A(x)|Ï‡_A(x)=(x^2+1)^2$<br>$T^2(v)=T(u)+T(iv)=2iu-vâ‰ 0â‡’(A^2+I)vâ‰ 0$<br>$â‡’m_A(x)âˆ¤(x^2+1)â‡’m_A(x)=(x^2+1)^2$</li>
<li>Eigenvalues of $A$ are roots $Â±i$ of $m_A(x)$.<br>By primary decomposition theorem,$V=\ker(A-iI)^2âŠ•\ker(A+iI)^2$<br>Take $v_1âˆˆ\ker(A-iI)^2âˆ–\ker(A-i)$, the matrix of $T$ over $\{(A-iI)v_1,v_1\}$ is $\pmatrix{i&1\\&i}$<br>Take $v_2âˆˆ\ker(A+iI)^2âˆ–\ker(A+iI)$, the matrix of $T$ over $\{(A+iI)v_2,v_2\}$ is $\pmatrix{-i&1\\&-i}$
so the Jordan Normal Form of $A$ is $\pmatrix{i&1\\&i\\&&-i&1\\&&&-i}$</li></ol></li></ol></li><li><ol type="a"><li><ol type="i"><li>Since $p(x),q(x)$ are coprime, $âˆƒa(x),b(x)âˆˆğ”½[x]:a(x)p(x)+b(x)q(x)=1$.<br>$âˆ€vâˆˆV:v=\underbrace{a(T)p(T)v}_{âˆˆ\ker q(T)}+\underbrace{b(T)q(T)v}_{âˆˆ\ker p(T)}$<br>And if $vâˆˆ\ker p(T)âˆ©\ker q(T)$, $v=a(T)p(T)v+b(T)q(T)v=0+0$</li>
<li>Set $ğ”½=â„,\dim V=2,p(x)=q(x)=x$. We need $\im TâŠ†\ker Tâ‡”T^2=0$<br>Set $T=\pmatrix{0&1\\0&0}$ then $T^2=0$, $\ker T=\span\{\pmatrix{1\\0}\}$, $\ker p(T)âˆ©\ker q(T)â‰ \{0\}$</li><li>Induct on $\deg m_T$. For $\deg m_T=1$, $m_T(x)=x-Î»_1â‡’T=Î»_1I$. Suppose it's true for $\deg m_T=n-1$.<br>Let $m_T(x)=\prod_{i=1}^n(x-Î»_i)$ for distinct $Î»_iâˆˆğ”½$.<br>Let $p(x)=(x-Î»_1)â‹¯(x-Î»_{n-1}),q(x)=x-Î»_n$.<br>Let $a(x),b(x)$ be minimal polynomial of the restriction of $T$ to $\ker p(T),\ker q(T)$, then $a(x)|p(x),b(x)|q(x)$. Also $a(x)b(x)$ annihilates $T$ on $V$, we have $p(x)q(x)|a(x)b(x)$, so $a(x)=p(x),b(x)=q(x)$. So the minimal polynomial of ${T|}_{\ker p(T)}$ is $p(x)$, a product of $n-1$ distinct linear factors. By induction hypothesis $\ker p(T)=\bigoplus_{i=1}^{n-1}\ker(T-Î»_iI)$.<br>By (a.i) $V=\ker p(T)âŠ•\ker q(T)â‡’V=\bigoplus_{i=1}^n\ker(T-Î»_iI)$, let $0â‰ v_iâˆˆ\ker(T-Î»_iI)$, $â„¬=\{v_1,â€¦,v_n\}$ then $\sideset{_â„¬}{_â„¬}{[T]}=\diag(Î»_1,â€¦,Î»_n)$.</li><li>Yes. If $T$ is diagonalisable, âˆƒ a basis â„¬ of $V$ such that $M=\sideset{_â„¬}{_â„¬}{[T]}$ is a diagonal matrix. Let $Î»_1,â€¦,Î»_n$ be the distinct diagonal entries of $M$, and let $f(x)=(x-Î»_1)â‹¯(x-Î»_n)$.<br>$âˆ€vâˆˆâ„¬$, $âˆƒkâˆˆ\{1,â€¦,n\}:Tv=Î»_kvâ‡’(T-Î»_kI)v=0â‡’f(T)v=(T-Î»_1I)â‹¯(T-Î»_kI)v=0$.
Since â„¬ spans $V$, $f(T)v=0âˆ€vâˆˆV$. Hence $m_T(x)$ divides $f(x)$, so $m_T(x)$ is a product of distinct linear factors.</li></ol></li><li><ol type="i"><li>Note that $T$ is invertible because $TT^{p-1}=T^{p-1}T=I$.<br>
Define $T$-conjugation map $C:â„°â†’â„°,Sâ†¦TST^{-1}$. Then $C$ is linear.<br>$âˆ€kâ‰¥1:C^k(S)=T^kST^{-k}$. Hence $C^p(S)=T^pST^{-p}=S$, so $C^p=\text{id}_â„°$,<br>$m_C(x)$ divides $x^p-1$ which factors over ğ”½ as $âˆ^{p-1}_{i=0}(x-Î¶^i)$. Now $Î¶$ has order dividing $p$ in $ğ”½^Ã—$. Since $Î¶â‰ 1$, this order is > 1. Since $p$ is prime, this order is $p$. Hence the elements $Î¶^0,Î¶^1,â€¦,Î¶^{p-1}$ are pairwise distinct. So $m_C(x)$ is a product of
distinct linear factors, by (a)(iii) $C:â„°â†’â„°$ is diagonalisable. The only possible eigenvalues of $C$ are $Î¶^i,i=0,â€¦,p-1$, and $\ker(C-Î¶^iI)=\{Sâˆˆâ„°:TST^{-1}=Î¶^iS\}=â„°(i)$.<br>
Hence $â„°=â„°(0) âŠ• â„°(1) âŠ• â‹¯ âŠ• â„°(p-1)$.</li><li>Since $T^p=I$, $Ïµ_i(S)=\sum_{j=0}^{p-1} Î¶^{i j} T^j S T^{-j}=\sum_{j=0}^{p-1} Î¶^{i j} C^j(S)$\[C(Ïµ_i(S))=\sum_{j=0}^{p-1} Î¶^{i j} C^{j+1}(S)=\sum_{j=0}^{p-1} Î¶^{i (j-1)} C^j(S)=Î¶^{-i}Ïµ_i(S)\]
So $Ïµ_i(S)âˆˆâ„°(-i)$.</li><li>If $U âˆˆ â„°(i)$ and $V âˆˆ â„°(j)$, then $C(U)=Î¶^i U$ and $C(V)=Î¶^j V$, so $C(U V)=T U V T^{-1}=T U T^{-1} T V T^{-1}=C(U) C(V)=Î¶^{i+j} U V$. Hence $U V âˆˆ â„°(i+j)$, which impies that $U^p âˆˆ â„°(0)$ for any $U âˆˆ â„°(i)$.<br>In particular, $Ïµ_i(S)^p âˆˆ â„°(0)$ because $Ïµ_i(S) âˆˆ â„°(-i)$ by the above. But $â„°(0)$ consists of precisely those $S âˆˆ â„°$ that commute with $T$.</li><li>Since $S â‰  0$ and the characteristic of $ğ”½$ is zero, it is enough to show that $Ïµ_0(S)+Ïµ_1(S)+â‹¯+Ïµ_{p-1}(S)=p S$.<br>The expression on the left hand side is equal to
$$
\sum_{i=0}^{p-1}\left(\sum_{j=0}^{p-1} Î¶^{i j} C^j(S)\right)=\sum_{j=0}^{p-1}\left(\sum_{i=0}^{p-1} Î¶^{i j}\right) C^j(S) .
$$
If $1 â©½ j â©½ p-1$, then because $pâˆ¤j$, $\left\{Î¶^0, Î¶^j, Î¶^{2 j}, â‹¯, Î¶^{(p-1) j}\right\}=\left\{Î¶^0, Î¶^1, â‹¯, Î¶^{p-1}\right\}$.<br>Hence $\sum_{i=0}^{p-1} Î¶^{i j}=\sum_{i=0}^{p-1} Î¶^i=0=\frac{Î¶^p-1}{Î¶-1}=0$. Hence $\sum_{i=0}^{p-1} Ïµ_i(S)=p S$ as claimed.</li></ol></li></ol></li><li><ol type="a"><li><ol type="i"><li>A complex vector space $V$ with a sesquilinear, conjugate symmetric, positive definite form $âŸ¨Â·,Â·âŸ©$ is called a complex inner product space.<br>
sesquilinear: $âŸ¨u+v,wâŸ©=âŸ¨u,wâŸ©+âŸ¨v,wâŸ©,âŸ¨\barÎ»v,wâŸ©=Î»âŸ¨v,wâŸ©=âŸ¨v,Î»wâŸ©$ for $v,wâˆˆV$<br>
conjugate symmetric: $\overline{âŸ¨v,wâŸ©}=âŸ¨w,vâŸ©$ for $v,wâˆˆV$<br>
positive definite: $âŸ¨v,vâŸ©>0$ for $vâ‰ 0$</li>
<li>$\tr(\overline{u+v}w)=\tr(\overline uw+\overline vw)=\tr(\overline uw)+\tr(\overline vw),\tr(\overline{\barÎ»v}w)=Î»\tr(\bar vw)=\tr(\bar vÎ»w)$<br>
$\overline{\tr(\bar vw)}=\tr(\overline{\bar vw})=\tr(\bar wv)$<br>
$\tr(\bar vv)=\sum_{i,j=1}^n\bar v_{ij}v_{ij}=\sum_{i,j=1}^n\left|v_{ij}\right|^2>0$ for $vâ‰ 0$.</li>
<li>Suppose $yâˆˆX^âŸ‚$, we'll show $y_{ij}=0$ for all $iâ‰¤j$.<br>Let $xâˆˆ M_n(â„‚)$ with $x_{ij}=\bar y_{ij}$ and all other entries 0, then$$xâˆˆXâ‡’\tr(\bar xy)=\left|y_{ij}\right|^2=0â‡’y_{ij}=0$$Conversely, if $y_{ij}=0$ for all $iâ‰¤j$, then $âˆ€xâˆˆX:$
\[\tr(\bar xy)=\sum_{i,j=0}^n\bar x_{ij}y_{ij}=\sum_{iâ‰¤j}\bar x_{ij}0+\sum_{i>j}0y_{ij}=0\]Hence $X^âŠ¥$ is the space of strictly lower-triangular matrices.</li></ol></li><li><ol type="i"><li>$Î±^*$ is uniquely defined by $âŸ¨v,Î±(w)âŸ©=âŸ¨Î±^*(v),wâŸ©$<br><i>Proof. </i>Let $\tilde Î±$ be another map satisfying $âŸ¨v,Î±(w)âŸ©=âŸ¨\tilde Î±(v),wâŸ©$. Then for all $v,wâˆˆV$
\begin{aligned}\left< Î±^*(v)-\tilde{Î±}(v),w\right>&=\left< Î±^*(v),w\right>-âŸ¨\tilde{Î±}(v),wâŸ©\\&=âŸ¨v,Î±(w)âŸ©-âŸ¨v,Î±(w)âŸ©\\&=0\end{aligned}But $âŸ¨Â·,Â·âŸ©$ is positive definite, hence $Î±^*(v)=\tilde Î±(v)âˆ€vâˆˆV$, so $Î±^*=\tilde Î±$.<hr>$âˆ€v,wâˆˆV:âŸ¨v,Î±(w)âŸ©=âŸ¨Î±^*(v),wâŸ©=âŸ¨v,(Î±^*)^*(w)âŸ©â‡’Î±(w)=(Î±^*)^*(w)â‡’Î±=Î±^*$</li><li>$âˆ€vâˆˆ\imÎ±âˆ©\kerÎ±^*:v=Î±(x)$ for some $x$ and $Î±^*(v)=0$\[âŸ¨v,vâŸ©=âŸ¨v,Î±(x)âŸ©=âŸ¨Î±^*(v),xâŸ©=0â‡’v=0\]</li><li>Lemma 1: $V=EâŠ•F$ where $E=\im(Î²^*)=\ker(Î±^*),F=\im(Î±)=\ker(Î²)$<br><i>Proof.</i> $âˆ€vâˆˆV,wâˆˆ\im(Î±)^âŸ‚:0=âŸ¨Î±(v),wâŸ©=âŸ¨v,Î±^*(w)âŸ©â‡’Î±^*(w)=0â‡’wâˆˆ\ker(Î±^*)$Conversely $âˆ€vâˆˆV,wâˆˆ\ker(Î±^*):âŸ¨Î±(v),wâŸ©=âŸ¨v,Î±^*(w)âŸ©=0â‡’wâˆˆ\im(Î±)^âŸ‚$<br>Therefore $\ker(Î±^*)=\im(Î±)^âŸ‚â‡’\ker(Î±^*)=\ker(Î²)^âŠ¥$<br>Similarly $\ker(Î²)=\im(Î²^*)^âŠ¥â‡’\im(Î±)=\im(Î²^*)^âŠ¥$<hr>Lemma 2: $\ker(Î±Î±^*)=\ker(Î±^*)$<br><i>Proof.</i> If $Î±Î±^*v=0$ then $0=âŸ¨Î±Î±^*v,vâŸ©=âŸ¨Î±^*v,Î±^*vâŸ©$, so $Î±^*v=0$<br>Conversely if $Î±^*v=0$ then $Î±Î±^*v=Î±0=0$<hr>Lemma 3: $Î±Î±^*:\im(Î±)â†’\im(Î±)$ is invertible.<br><i>Proof.</i> $\ker(Î±Î±^*)âˆ©\im(Î±)=\ker(Î±^*)âˆ©\im(Î±)=\{0\}$ by lemma 2 and (ii).<br>By rank-nullity theorem, $Î±Î±^*:\im(Î±)â†’\im(Î±)$ is invertible.<hr>Lemma 4: $Î²^*Î²$ restricted to $\im(Î²^*)$ is invertible.<br><i>Proof.</i> $\ker(Î²^*Î²)âˆ©\im(Î²^*)=\ker(Î²)âˆ©\im(Î²^*)=\{0\}$ by lemma 2 and (ii).<br>By rank-nullity theorem, $Î²^*Î²:\im(Î²^*)â†’\im(Î²^*)$ is invertible.<hr>$Î³$ has matrix $\begin{pmatrix}Î±Î±^* & 0 \\ 0 & Î²^*Î²\end{pmatrix}$ with respect to the decomposition in Lemma 1, where both diagonal entries are invertible (as maps $Eâ†’E$ and $Fâ†’F$). Thus $Î³$ is invertible.</li></ol></li></ol></li></ol>