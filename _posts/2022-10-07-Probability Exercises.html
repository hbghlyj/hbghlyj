---
mathjax: true
tag: Probability
excerpt: College exam questions
---
<ol><li>
<ol type="a"><li>Explain what it means for a sequence of random variables $Y_1, Y_2, \ldots$ to converge to a random variable $Y$
<ol type="i"><li>in probability;
</li>
<li>in distribution.</li></ol>
</li>
<li> Prove that convergence in probability implies convergence in distribution.</li></ol>
<b>Proof.</b> <a href="https://en.wikipedia.org/wiki/Proofs_of_convergence_of_random_variables#Proof_for_the_case_of_scalar_random_variables">Wikipedia</a>
Recall that in order to prove convergence in distribution, one must show that the sequence of cumulative distribution functions converges to the $F_X$ at every point where $F_X$ is continuous. Let $a$ be such a point. For every Îµ > 0, due to the preceding lemma, we have:
\begin{aligned}\Pr(X_n\leq a)&\leq \Pr(X\leq a+Îµ)+\Pr(\left|X_n-X\right|>Îµ)\\\Pr(X\leq a-Îµ)&\leq \Pr(X_n\leq a)+\Pr(\left|X_n-X\right|>Îµ)\end{aligned}
So, we have
$$ \Pr(X\leq a-Îµ)-\Pr\left(\left|X_n-X\right|>Îµ\right)\leq \Pr(X_n\leq a)\leq \Pr(X\leq a+Îµ)+\Pr\left(\left|X_n-X\right|>Îµ\right). $$
Taking the limit as $n â†’ âˆ$, we obtain:
$$ F_{X}(a-Îµ)\leq \lim _{n\to \infty }\Pr (X_n\leq a)\leq F_{X}(a+Îµ), $$
where $F_X(a) =\Pr(X â‰¤ a)$ is the cumulative distribution function of $X$. This function is continuous at $a$ by assumption, and therefore both $F_X(a-Îµ)$ and $F_X(a+Îµ)$ converge to $F_X(a)$ as $Îµ â†’ 0^+$. Taking this limit, we obtain
$$ \lim _{n\to \infty }\Pr(X_n\leq a)=\Pr(X\leq a), $$
which means that $\{X_n\}$ converges to $X$ in distribution. 
</li>
<li>Let $\left(Î±_k, k=1,2,3, \ldots\right)$ be a probability distribution on the positive integers (that is, $Î±_kâ©¾0$ for all $k$, and $\sum Î±_k=1$).
Consider a Markov chain $X=\left(X_n, nâ©¾0\right)$ with state space $\{1,2,3, \ldots\}$ whose transition probabilities are given by
$$
\begin{aligned}
p_{i, i-1} & =1 \text { for } i>1, \\
p_{1, k} & =Î±_k \text { for } k=1,2,3, \ldots .
\end{aligned}
$$
That is, the chain always steps down by 1 unless it is in state 1; when it is in state 1, it chooses its next state according to the distribution $\left(Î±_k\right)$.
<ol type="a"><li>What does it mean for a Markov chain to be irreducible?<p>It has only 1 communicating class i.e. âˆ€state $i,j$ both $iâ†’j$ and $jâ†’i$ hold.</p>
</li>
<li>Under what condition on the distribution $\left(Î±_k\right)$ is the chain $X$ described above irreducible?<p>âˆƒ infinitely many $k$ such $Î±_k>0$ i.e. $âˆ€n,âˆƒk>n$ such that $Î±_k>0$.</p>
</li>
<li>Suppose the chain is irreducible. Under what condition on the distribution $\left(Î±_k\right)$ is the chain aperiodic?<p>$1=\gcd\{k|Î±_k>0,k=1,2,â€¦\}$</p>
</li>
<li>Suppose the chain is irreducible. By considering the mean return time to 1, or otherwise, determine under what condition on the distribution $\left(Î±_k\right)$ the chain is positive recurrent. Give an interpretation of this condition in terms of a well-known function of the distribution $\left(Î±_k\right)$.<p>$m_1=Î±_1â‹…m_1+Î±_2â‹…m_2+Î±_3â‹…m_3+â‹¯=Î±_1â‹…0+Î±_2â‹…1+Î±_3â‹…2+â‹¯&lt;âˆ$</p><p>interpretation: mean of $(Î±_k)$ is finite</p>
</li>
<li>Find the stationary distribution of the chain if $Mâ©¾2$ and
$$
Î±_k= \begin{cases}\frac1M & \text { for } k=1,2, \ldots, M, \\ 0 & \text { for } k>M .\end{cases}
$$
[You may assume that the stationary distribution is unique. Note that if $\left(Ï€_i, iâ©¾1\right)$ is the stationary distribution then $Ï€_i=0$ for all $i>M$.]<p>\[A=\pmatrix{0&\frac1m&\frac1m&â‹¯&\frac1m\\1&0\\&1&0\\&&1&0\\&&&â‹±}\]Ï€ is stationary if $Ï€A=Ï€â‡’\cases{Ï€_M=\frac1MÏ€_1\\Ï€_{M-1}=\frac1MÏ€_1+Ï€_M\\Ï€_{M-2}=\frac1MÏ€_1+Ï€_{M-1}\\â‹®\\Ï€_2=\frac1MÏ€_1+Ï€_2}â‡’Ï€_{M-k}=\frac{k+1}MÏ€_1$</p><p>$âˆ‘Ï€_k=1â‡’\frac{Ï€_1}Mâˆ‘(k+1)=1â‡’Ï€_1=\frac2{M+1},Ï€_{M-k}=\frac{k+1}Mâ‹…\frac2{M+1}$</p>
</li>
<li>Now let
$$
Î±_k= \begin{cases}\left(\frac{1}{2}\right)^{k / 2} & \text { for } k \text { even } \\ 0 & \text { for } k \text { odd }\end{cases}
$$
i.e. $\left(Î±_k, kâ©¾1\right)=\left(0, \frac{1}{2}, 0, \frac{1}{4}, 0, \frac{1}{8}, \ldots\right)$.
By considering the mean return time to 1, or otherwise, find the limit points of the sequence $\Pr\left(X_n=1 \mid X_0=1\right)$ as $nâ†’âˆ$.<p>Mean return time$$\sum_{kâ‰¥1}kÎ±_k=2Ã—\text{mean Geometric}(\frac12)=4$$</p><p>limit points: odd times at an even number\[\Pr(X_n=1|X_0=1,n\text{ odd})=0\]$Y_k=X_{2k}$ lives on $\{1,3,5,7,â€¦\}$ mean return time to 1 for $y$ is 2\[\lim_{nâ†’âˆ}\Pr(Y_n=X_{2n}=1|X_0=1)=\frac1{\text{mean return time }yâ†’1}=\frac12\]</p></li></ol>
</li>
<li>
<ol type="a"><li>Define the moment generating function of a random variable. If $X_i, iâ©¾1$ are i.i.d. with moment generating function $M(t)$, find the moment generating function of the random variable $S_n=X_1+\cdots+X_n$ in terms of $M(t)$.<p>$ğ”¼(e^{tX})=M_X(t)$</p><p>$S_n=\sum_{i=1}^nX_i$</p>\begin{align*}M_{S_n}(t)&=ğ”¼(e^{t\sum_{i=1}^nX_i})\\&=ğ”¼(\prod_{i=1}^ne^{tX_i})\\&=\prod_{i=1}^nğ”¼(e^{tX_i})\\&=M_{X_1}(t)^n\end{align*}</li>
For the rest of the question, let $X_i, iâ©¾1$ be i.i.d. with distribution given by
$$
\Pr\left(X_i=1\right)=\Pr\left(X_i=-1\right)=1 / 2
$$
and let $S_n=X_1+\cdots+X_n$ as above.
<li>State Chebyshev's inequality, for a general random variable with mean $\mu$ and variance $\sigma^2$. Use it to give an upper bound on the quantity $\Pr\left(\left|S_n\right|â©¾a n\right)$ where $a>0$.<p>Putting $ğ”¼[X_i]=0,\operatorname{Var}(X_i)=1$ into Chebyshev $\Pr(|Y-ğ”¼(Y)|â‰¥Ïµ)â‰¤\frac{\operatorname{Var}(Y)}{Ïµ^2}$</p><p>$ğ”¼(S_n)=âˆ‘ğ”¼(X_i)=0,\operatorname{Var}(S_n)=n$</p><p>$\Pr(|S_n|â‰¥an)â‰¤\frac{\operatorname{Var}(S_n)}{a^2n^2}=\frac1{a^2n}$</p>
</li>
<li>Find the moment generating function $M(t)$ of $X_i$.<p>$M(t)=\frac12e^{-t}+\frac12e^t$</p>By comparing the Taylor expansions of $M(t)$ and $\exp \left(t^2 / 2\right)$ or otherwise, show that
$$
M(t) â©½ \exp \left(t^2 / 2\right) \text { for all } t .
$$
<p>Since $\frac1{(2k)!}â‰¤\frac1{k!2^k}$</p>
Use part (a) to deduce a corresponding bound on the moment generating function of $S_n$.\[M_{S_n}(t)=M(t)^nâ‰¤\exp \left(nt^2 / 2\right)\]
</li>
<li>Let $a>0$. By applying Markov's inequality to an appropriate random variable, use the bound in (b)(ii) to show that for any $t$
$$
\Pr\left(S_nâ©¾a n\right) â©½ \exp \left(n\left\{\frac{t^2}{2}-a t\right\}\right) .
$$
Deduce that
$$
\Pr\left(\left|S_n\right|â©¾a n\right) â©½ 2 \exp \left(-\frac{n a^2}{2}\right)
$$<p>$\Pr\left(S_nâ©¾a n\right)â©½\exp(n(\frac{t^2}2-at))$</p><p>Markov's inequality $\Pr\left(S_nâ©¾a n\right)=\Pr\left(e^{tS_n}â©¾e^{ta n}\right)â‰¤\frac{ğ”¼(e^{tS_n})}{e^{tan}}â‰¤\frac{e^{{n\over2}t^2}}{e^{tan}}$</p><p>$X$ symmetric about 0 â‡’ $S$ symmetric about 0 â‡’ $\Pr(S_nâ‰¥an)=\Pr(S_nâ‰¤-an)$</p></li></ol>
</li></ol>