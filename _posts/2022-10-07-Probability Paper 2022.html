---
mathjax: true
tag: Probability
excerpt: A8 Probability Paper 2022
---
<ol><li>
<ol type="a"><li>Let $Z_n,nâ‰¥1$, be random variables and $câˆˆâ„$. Show that $Z_nâ†’c$ in probability if and only if $Z_nâ†’c$ in distribution.</li><li>Fix $Î»âˆˆ(0,âˆ)$. For each $râˆˆ(0,âˆ)$, consider a random variable $X_r$ with probability density function
$$
f_{r,Î»}(x)=\begin{cases}\frac{1}{Î“(r)}x^{r-1}Î»^r e^{-Î» x},&xâˆˆ(0,âˆ),\\0,&\text { otherwise. }\end{cases}
$$
Recall that this means that $X_r$ has the Gamma distribution with shape parameter $r$ and rate parameter $Î»$.
<ol type="i">
<li>Carefully derive the moment generating function of $X_r$.</li>
<li>
Show that $X_r/r$ converges in distribution as $râ†’âˆ$. Does this convergence hold in probability?<br>
[You may use standard theorems about moment generating functions without proof.]
</li></ol></li>
<li><ol type="i"><li>Define the Poisson process $\left(N_t,tâ‰¥0\right)$ of rate $Î»âˆˆ(0,âˆ)$ in terms of properties of its increments over disjoint time intervals.</li><li>Show that the first arrival time $T_1=\inf\left\{tâ‰¥0:N_t=1\right\}$ is exponentially distributed with parameter $Î»âˆˆ(0,âˆ)$.</li><li>Show that $T_n=\inf\left\{tâ‰¥0:N_t=n\right\}$ has a Gamma distribution for all $nâ‰¥1$.<br>
[If you use the inter-arrival time definition of the Poisson process, you are expected to prove that it is equivalent to the definition given in (i).]</li><li>Let $R_n,nâ‰¥1$, be independent Gamma$(n,Î»)$ random variables.<br>
Let $Y_t=\#\left\{nâ‰¥1:R_nâ‰¤t\right\},tâ‰¥0$. Show that $Y_t$ is not a Poisson process with rate $Î»$, but does satisfy $â„™\left(Y_t&lt;âˆ\text{ for all }tâ‰¥0\right)=1$.<br>
[Hint: Let $B_n=1_{\left\{R_nâ‰¤t\right\}}$ and write $Y_t=\sum_{nâ‰¥1}B_n$.]</li>
</ol></li></ol></li>
<li><ol type="a">
<li>State the Central Limit Theorem.</li>
<li>Let $(R,S)$ be a pair of random variables with joint probability density function
$$
f(r,s)=\begin{cases}\frac{1}{4}e^{-{|s|}},&(r,s)âˆˆ[-1,1]Ã—â„\\0,&\text { otherwise. }\end{cases}
$$
Also consider independent identically distributed random variables $\left(R_n,S_n\right),nâ‰¥1$, with the same joint distribution as $(R,S)$.<ol type="i"><li>Find the marginal probability density functions of $R$ and $S$.</li><li>For any $sâˆˆâ„$, determine
$$
\lim _{nâ†’âˆ}â„™\left(\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_nâ‰¤s\right)
$$</li><li>For any $r,sâˆˆâ„$, show that
$$
\lim _{nâ†’âˆ}â„™\left(\frac{1}{\sqrt{n\operatorname{var}(R)}}\sum_{k=1}^n R_nâ‰¤r,\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_nâ‰¤s\right)=â„™(Wâ‰¤r,Zâ‰¤s)
$$
for a pair of random variables $(W,Z)$ whose joint distribution you should determine.</li></ol>
</li><li>Consider the transformation $T:â„^2â†’â„^2$ given by $T(x,y)=(x-y,x+y)$. Let $(R,S)$ be as in (b) and $(X,Y)$ such that $(R,S)=T(X,Y)$.<ol type="i"><li>Derive the joint probability density function of $(X,Y)$.</li><li>Find the marginal probability density functions of $X$ and $Y$.</li><li>Find the correlation of $X$ and $Y$.</li></ol></li></ol></li>
<li><ol type="a"><li>Consider a Markov chain on a countable state space $S$ and let $i âˆˆ S$.
<ol type="i"><li>Define the notions of recurrence and positive recurrence of $i$.
</li>
<li>Suppose that $i$ is positive recurrent. State, without proof, the ergodic theorem for the long-term proportion of time the Markov chain spends in state $i$.</li></ol>
</li>
<li>An urn contains a total of $N â‰¥ 2$ balls, some white and the others black. Each step consists of two parts. A first ball is chosen at random and removed. A second ball is then chosen at random from those remaining. It is returned to the urn along with a further ball of the same colour. Denote by $Y_n$ the number of white balls after $n â‰¥ 0$ steps.
<ol type="i"><li>Explain briefly why $\left(Y_n, n â‰¥ 0\right)$ is a Markov chain and determine its state space and transition matrix.
</li>
<li>Determine the communicating classes of this Markov chain and say whether their states are recurrent, and whether they are aperiodic. Justify your answers.
</li>
<li>Find all stationary distributions of this Markov chain.</li></ol>
</li>
<li>Now consider a Markov chain $\left(Z_n, n â‰¥ 0\right)$, on $I=\{0,1,2, â€¦, N\}$ with the transition matrix $P$ whose non-zero entries are
$$
p_{k, j}= \begin{cases}\frac{N-k}{N} \frac{k+1}{N+1} & \text { if } j=k+1, \\ \frac{N-k}{N} \frac{N-k}{N+1}+\frac{k}{N} \frac{k}{N+1} & \text { if } j=k, \\ \frac{k}{N} \frac{N-k+1}{N+1} & \text { if } j=k-1 .\end{cases}
$$
<ol type="i"><li>Show that the uniform distribution is stationary for this Markov chain. Hence, or otherwise, determine all stationary distributions of this Markov chain.
</li>
<li>For a state $k âˆˆ I$, consider the successive visits
$$
V_1^{(k)}=\inf \left\{n â‰¥ 1: Z_n=k\right\} â€ƒ \text { and } â€ƒ V_{m+1}^{(k)}=\inf \left\{n â‰¥ V_m^{(k)}+1: Z_n=k\right\}, â€ƒ m â‰¥ 1 .
$$
Explain why visits to $k$ occur in groups of independent geometrically distributed consecutive visits, and determine the parameter of this geometric distribution.
</li>
<li>Determine the expected time between two groups of visits to state $k$.
</li>
<li>Is the following statement true or false? â€˜For any two states $k_1 â‰  k_2$, there is, on average, one visit to $k_2$ between the first and second visits to $k_1$.â€™ Provide a proof or counterexample.</li></ol></li></ol></li></ol>
<h1>Solution</h1>
<ol><li><ol type="a"><li>[Sheet 1 Q6] If $Z_n\overset dâ†’c$, since $F_c$ is continuous on $â„âˆ–\{c\}$, for any $Ïµ>0$,
\begin{align*}
â„™\left(\left|Z_n-c\right|â‰¥Ïµ\right) &=â„™\left(Z_nâ‰¤c-Ïµ\right) +â„™\left(Z_nâ‰¥c+Ïµ\right)\\
&= F_{Z_n}(c-Ïµ)+1-F_{Z_n}(c+Ïµ)\\&â†’0+1-1=0.\text{ So }Z_n\overset Pâ†’c.\end{align*}If $Z_n\overset Pâ†’c$, we need to prove for any $Ïµ>0$, $F_{Z_n}(c-Ïµ)â†’0,F_{Z_n}(c+Ïµ)â†’1$.\[â„™(Z_nâ‰¤c-Ïµ)â‰¤â„™(\left|Z_n-c\right|>Ïµ)â†’0\]So $F_{Z_n}(c-Ïµ)â†’0$\[â„™(Z_nâ‰¥c+Ïµ)â‰¤â„™(\left|Z_n-c\right|>Ïµ)â†’0\]So $F_{Z_n}(c+Ïµ)â†’1$</li>
<li><ol type="i"><li>[Sheet 2 Q2] for $t&lt;Î»$,\begin{align*}M_{X_r}(t)&=âˆ«_0^âˆe^{tx}\frac1{Î“(r)} Î»^r x^{r-1} e^{-Î» x}dx\\
&=\frac{Î»^r}{(Î»-t)^r}âˆ«_0^âˆ\underbrace{\frac1{Î“(r)}(Î»-t)^r x^{r-1} e^{-(Î»-t)x}}_{\text{p.d.f for }Î“(r, Î»-t)}dx\\
&=\left(1-\frac tÎ»\right)^{-r}\end{align*}</li><li>For $t&lt;rÎ»:M_{X_r/r}(t)=M_{X_r}\left(t\over r\right)=\left(1-\frac t{rÎ»}\right)^{-r}â†’e^{t/Î»}$<br>
By convergence theorem $X_r/r\overset dâ†’1/Î»$.<br>
By (a), $X_r/r\overset pâ†’1/Î»$.
</li></ol></li><li><ol type="i"><li>The counting process $N_t, t â‰¥ 0$ is a Poisson process of rate $Î»$ if:<ol style="padding:0"><li>$N_0=0$.</li><li>If $\left(s_1, t_1\right),\left(s_2, t_2\right), â€¦,\left(s_k, t_k\right)$ are disjoint intervals in $â„_{+}$, then the increments $N\left(s_1, t_1\right]$, $N\left(s_2, t_2\right], â€¦, N\left(s_k, t_k\right]$ are independent, where $N\left(s_i, t_i\right]=N_{t_i}-N_{s_i}$.</li><li>For any $s&lt;t$, the increment $N(s, t]$ has Poisson distribution with mean $Î»(t-s)$.</li></ol></li>
<li>$0=N_0â‰¤N_1â‰¤â‹¯â‰¤N_t$, so $T_1>tâ‡”N_0=â‹¯=N_t=0â‡”N_t=0$<br>
$N_tâˆ¼\operatorname{Poisson}(Î»t)â‡’â„™(T_1>t)=â„™(N_t=0)=e^{-Î»t}â‡’T_1âˆ¼\operatorname{Exp}(Î»)$.</li>
<li>Let $X_nâˆ¼\operatorname{Gamma}(n,Î»)$. By <a href="https://www2.math.upenn.edu/~kazdan/361F15/Notes/Taylor-integral.pdf">Taylor's formula with integral remainder</a>,
\[e^{Î»t}=\sum_{k=0}^{n-1}\frac{(Î»t)^k}{k!}+âˆ«_0^t\frac{1}{(n-1)!}x^{n-1}Î»^n e^{Î»(t-x)}dx\]Therefore
\begin{split}â„™(X_nâ‰¤t)&=âˆ«_0^t\frac{1}{(n-1)!}x^{n-1}Î»^n e^{-Î»x}dx\\&=1-e^{-Î»t}\sum_{k=0}^{n-1}\frac{(Î»t)^k}{k!}\end{split}
On the other hand, $â„™(T_nâ‰¤t)=â„™(N_tâ‰¥n)=1-e^{-Î»t}\sum_{k=0}^{n-1}\frac{(Î»t)^k}{k!}$. So $T_n=X_n$.
</li>
<li>In Poisson process $â„™(T_1>T_2)=0$ so $T_1,T_2$ are not independent, but $R_1,R_2$ are independent, so $Y_t$ is not a Poisson process.<br>Let $B_n=1_{\left\{R_nâ‰¤t\right\}}$ and write $Y_t=\sum_{nâ‰¥1}B_n$
\begin{split}ğ”¼[Y_t]&=\sum_{nâ‰¥1}ğ”¼[B_n]\\&=\sum_{nâ‰¥1}â„™(R_nâ‰¤t)\\&=\sum_{nâ‰¥1}âˆ«_0^t\frac{1}{(n-1)!}x^{n-1}Î»^n e^{-Î»x}dx\\&=âˆ«_0^t\sum_{nâ‰¥1}\frac{1}{(n-1)!}x^{n-1}Î»^n e^{-Î»x}dx\\&=âˆ«_0^tÎ»dx=Î»t&lt;âˆ\end{split}So $â„™\left(Y_t&lt;âˆ\text{ for all }tâ‰¥0\right)=1$
</li></ol></li></ol></li><li><ol type="a"><li>Let $X_1,X_2,â€¦$ be i.i.d. random variables with mean $Î¼$ and variance $Ïƒ^2âˆˆ(0,âˆ)$.<br>Let $S_n=X_1+X_2+â‹¯+X_n$. Then ${S_n-nÎ¼\over\sqrt nÏƒ}\stackrel dâ†’N(0,1)$ as $nâ†’âˆ$.</li><li><ol type="i"><li>$f_R(r)=âˆ« f(r,s)\mathrm{~d}s=2âˆ«_0^âˆ\frac14e^{-s}\mathrm{~d}s=\frac12$ for $râˆˆ[-1,1]$; otherwise $f_R(r)=0$<br>$f_S(s)=âˆ«f(r,s)\mathrm{~d}r=âˆ«_{-1}^1\frac14e^{-{|s|}}\mathrm{~d}r=\frac12e^{-{|s|}}$</li><li>$ğ”¼[S]=âˆ«_{-âˆ}^âˆs\frac12e^{-{|s|}}\mathrm{~d}s=0$, by symmetry$$\operatorname{var}(S)=ğ”¼\left[S^2\right]=2âˆ«_0^âˆs^2\frac{1}{2}e^{-s}d s=2&lt;âˆ$$$S_n$ are iid with mean 0, and variance 2, by CLT $\frac1{\sqrt{2n}}\sum_{k=1}^n S_n\stackrel dâ†’Z$. For all $sâˆˆâ„$$$â„™\left(\frac1{\sqrt{2n}}\sum_{k=1}^n S_nâ‰¤s\right)â†’â„™(Zâ‰¤s),\text{â€ƒwhere $Zâˆ¼N(0,1)$.}$$</li><li>$Râˆ¼U[-1,1]â‡’ğ”¼(R)=0$, $\operatorname{var}(R)=\frac13&lt;âˆ$. $R_n$ are iid, by CLT\[â„™\left(\sqrt{\frac3n}\sum_{k=1}^n R_nâ‰¤r\right)â†’â„™(Wâ‰¤r),\text{â€ƒwhere $Wâˆ¼N(0,1)$.}\]$\operatorname{cov}(R,S)=ğ”¼[RS]=âˆ«rsf(r,s)=2âˆ«_{-âˆ}^âˆ\frac14se^{-{|s|}}\mathrm{~d}s=0$, so $R,S$ are independent.\begin{multline*}
â„™\left(\frac{1}{\sqrt{n\operatorname{var}(R)}}\sum_{k=1}^n R_nâ‰¤r,\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_nâ‰¤s\right)\\=â„™\left(\frac{1}{\sqrt{n\operatorname{var}(R)}}\sum_{k=1}^n R_nâ‰¤r\right)â„™\left(\frac{1}{\sqrt{n\operatorname{var}(S)}}\sum_{k=1}^n S_nâ‰¤s\right)\\â†’â„™(Wâ‰¤r)â„™(Zâ‰¤s)=â„™(Wâ‰¤r,Zâ‰¤s)\text{,â€ƒwhere }W,Z\stackrel{\text{iid}}âˆ¼N(0,1)
\end{multline*}</li></ol></li><li><ol type="i"><li>$\left|\frac{âˆ‚(R,S)}{âˆ‚(X,Y)}\right|=\begin{vmatrix}1&-1\\1&1\end{vmatrix}=2$. By the transformation formula, $(X,Y)$ has joint pdf\[f_{X,Y}(x,y)=2f_{R,S}(T(X,Y))=\begin{cases}\frac{1}{2} e^{-{|x+y|}} & \text { if }{|x-y|}â‰¤1 \\ 0 & \text { otherwise. }\end{cases}\]</li><li>Fix $y$ then $f_{X,Y}(x,y)>0â‡”xâˆˆ[y-1, y+1]$. (horizontal section of the region)<br>
If ${|y|}â‰¥\frac12$, $x+yâ‰¥0$ for $xâˆˆ[y-1, y+1]$,\begin{aligned}
f_Y(y) & =\int_{y-1}^{y+1} \frac{1}{2} e^{-{|x+y|}} d x\\
& =\frac{1}{2}\left(e-e^{-1}\right) e^{-2{|y|}}
\end{aligned}If ${|y|}â‰¤\frac12$, $x+y$ changes sign at $x=-y$,\begin{aligned}
f_Y(y) &=\int_{y-1}^{-y}\frac{1}{2} e^{x+y} d x+\int_{-y}^{y+1}\frac{1}{2} e^{-x-y} d x\\
& =1-e^{-1}\cosh (2 y)
\end{aligned}{% latex %}\usepackage{tikz}
\begin{tikzpicture}[scale=2,font=\footnotesize]
    \fill[gray!20] (-3,-2) -- (-1,-2) -- (3,2) -- (1,2) -- cycle;
    \draw (-3,0) -- (3,0) node[right] {$x$}(0,-2) -- (0,2) node[above] {$y$};
    \draw[dashed] (-2,2)--(2,-2)(1.5,-1.2)node[right,red]{$x+y>0$}(1.2,-1.5)node[left,red]{$x+y&lt;0$};
    \foreach \y in {.7,.5,.3}\draw[red,thick](\y-1,\y)--(\y+1,\y)node[right]{$0\y$};
    \foreach \x in {-3,-2,-1,1,2,3}
      \draw (\x,-0.05) -- (\x,0.05) (\x,0)node[above] {$\x$};
    \foreach \y in {-2,-1,1,2}
      \draw (-0.05,\y) -- (0.05,\y) (0,\y)node[right] {$\y$};
  \end{tikzpicture}{% endlatex %}
</li><li>In b(iii) we found cov$(R,S)=0$,$$\operatorname{cov}(X, Y)=\operatorname{cov}\left(\frac{R+S}2, \frac{S-R}2\right)=\frac{1}{4}(\operatorname{var}S-\operatorname{var}R)=\frac14(2-\frac13)=\frac{5}{12}$$correlation coefficient\[\frac{\operatorname{cov}(X, Y)}{\sqrt{\operatorname{var}(X)\operatorname{var}(Y)}}=\frac{\frac5{12}}{\sqrt{\frac7{12}\frac7{12}}}=\frac57\]</li></ol></li></ol></li><li><ol type="a"><li><ol type="i"><li>$i$ is recurrent$â‡”â„™_i(X_n=i\text{ for some }nâ‰¥1)=1â‡”â„™_i(\inf\{nâ‰¥1:X_n=i\}&lt;âˆ)=1$<br>$i$ is positive recurrent$â‡”m_i=ğ”¼_i(\inf\{nâ‰¥1:X_n=i\})&lt;âˆ$</li><li>Let $V_i(n)$ be the number of visits to state $i$ before time $n$. Then for any initial distribution, ${V_i(n)\over n}â†’\frac1{m_i}$ almost surely.</li></ol></li><li><ol type="i"><li>$(Y_n,nâ‰¥0)$ satisfy Markov property: For any $t_0$, the distribution of $(Y_n, n>n_0)$ is independent of $(Y_n, n â‰¤ n_0)$.<br>Total number of balls is $N$, so $Y_n$ has $N+1$ states $0,â€¦,N$. If $Y_n=k$, four cases
<style>
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-style:solid;border-width:1px;}
</style>
<table class="tg">
    <tr>
    <td>WWâ†’WW</td>
    <td>$Y_{n+1}=k$</td>
    <td>$\frac kN\frac{k-1}{N-1}$</td>
    </tr>
    <tr>
    <td>WBâ†’BB</td>
    <td>$Y_{n+1}=k-1$</td>
    <td>$\frac kN\frac{N-k}{N-1}$</td>
    </tr>
    <tr>
    <td>BWâ†’WW</td>
    <td>$Y_{n+1}=k+1$</td>
    <td>$\frac{N-k}N\frac{k}{N-1}$<br></td>
    </tr>
    <tr>
    <td>BBâ†’BB</td>
    <td>$Y_{n+1}=k$</td>
    <td>$\frac{N-k}N\frac{N-k-1}{N-1}$</td>
    </tr>
</table>
So the non-zero entries of the transition matrix are
$$
p_{k, j}= \begin{cases}\frac{k}{N} \frac{N-k}{N-1} & \text { if } j=k-1,k>0\\\frac{k}{N} \frac{k-1}{N-1} +\frac{N-k}{N} \frac{N-k-1}{N-1}& \text { if } j=k\\\frac{N-k}{N} \frac{k}{N-1} & \text { if } j=k+1,k&lt;N\end{cases}
$$</li><li>$0,N$ are absorbing states (aperiodic, recurrent)<br>The rest form a communicating class, since $p_{i, i-1}=p_{i, i+1}>0$ for $1 â‰¤ i â‰¤ N-1$<br>This is transient, since it has positive probability to escape to $0,N$</li>
<li>For Ï€ to be stationary, we need\begin{eqnarray*}Ï€_0&=&Ï€_0+\frac1NÏ€_1â‡’Ï€_1=0\\Ï€_N&=&Ï€_N+\frac1NÏ€_{N-1}â‡’Ï€_{N-1}=0\\Ï€_i&=&p_{i-1,i}Ï€_{i-1}+p_{i,i}Ï€_i+p_{i+1,i}Ï€_{i+1}â‡’Ï€_i=0\text{ for }0&lt;i&lt;N\end{eqnarray*}So $(Î», 0,0, â€¦, 0,1-Î»), 0 â‰¤ Î» â‰¤ 1$ are stationary distributions.</li></ol></li>
<li><ol type="i"><li>For Ï€ to be stationary, we need\begin{split}
Ï€_0&=Ï€_0p_{0,0}+Ï€_1p_{1,0}\\
Ï€_N&=Ï€_{N-1}p_{N-1,N}+Ï€_Np_{N,N}\\
Ï€_j&=Ï€_{j-1} p_{j-1, j}+Ï€_j p_{j, j}+Ï€_{j+1} p_{j+1, j}\text{ for }0 â‰¤ j â‰¤ N
\end{split}
We check the uniform distribution $Ï€_i=1 /(N+1), 0 â‰¤ i â‰¤ N$ is a solution.\begin{array}l
\frac1{N+1}=\frac1{N+1}\frac{N}{N+1}+\frac1{N+1}\frac1{N+1}\\
\frac1{N+1}=\frac1{N+1}\frac1{N+1}+\frac1{N+1}\frac{N}{N+1}\\
\frac1{N+1}=\frac1{N+1}\frac{k}{N} \frac{N-k+1}{N+1}+\frac1{N+1}\left(\frac{N-k}{N} \frac{N-k}{N+1}+\frac{k}{N} \frac{k}{N+1}\right)+\frac1{N+1} \frac{N-k}{N} \frac{k+1}{N+1}\text{ for }0 â‰¤ j â‰¤ N\end{array}
This chain is irreducible: $p_{0,1}>0;p_{N,N-1}>0;p_{i, i-1}=p_{i, i+1}>0\text{ for }1 â‰¤ i â‰¤ N-1$.<br>By uniqueness theorem, uniform distribution is the only stationary distribution.</li>
<li>Let $i$ be the number of consecutive visits to $k$. By the Markov property,\begin{split}&â„™(Z_{n+i}â‰ k,Z_{n+i-1}=k,â€¦,Z_{n+1}=k|Z_n=k)\\&=â„™(Z_{n+i}â‰ k|Z_{n+i-1}=k)â€¦â„™(Z_{n+1}=k|Z_n=k)\\&=(1-p_{k,k})p_{k,k}^{i-1}\end{split}so $i$ has geometric distribution with success probability $1-p_{k,k}$</li>
<li>By Theorem 6.3(b) $1/Ï€_k=m_k$, $m_k$ is the mean return time to state $k$.<br>By the Law of Total Probability,
\begin{split}
N+1&=ğ”¼_k\left[V_1^{(k)} âˆ£ Z_1â‰ k\right] â„™(Z_1â‰ k)+ğ”¼_k\left[V_1^{(k)} âˆ£ Z_1=k\right] â„™\left(Z_1=k\right)\\&=x (1-p_{k,k})+p_{k,k}\\&â‡’x=1+\frac N{1-p_{k,k}}
\end{split}
</li>
<li>I don't understand</li></ol></li></ol></li></ol>